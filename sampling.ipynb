{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3bcd5a-92e8-4f65-b186-77af7e536738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\diffusion\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import easydict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "############################### Change this term for your input dataset ###############################\n",
    "\n",
    "numofcluster = 5\n",
    "clustertype = \"1st\"\n",
    "\n",
    "ckptname = f\"model_1m_{clustertype}={numofcluster}_sig\"  ## write xxxx when your \"main\" ckpt name is xxxx.ckpt\n",
    "mode = \"sig\"  ## if your autoencoder use sigmoid layer in training step, write sig, else anything.\n",
    "\n",
    "path = \"./data/\"\n",
    "filename1 = path + f\"ml_1m_user_mov_{clustertype}_cluster={numofcluster}.csv\"\n",
    "new_data = pd.read_csv(filename1)\n",
    "\n",
    "#######################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b393cbfd-2d07-4213-a8cf-4c2ab3c332f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(schedule, start, end, n_timestep):\n",
    "    \"\"\"\n",
    "    입력한 scheduling 방식에 따라 forward process에서 사용되는 beta를 만들어주는 함수\n",
    "    input : \n",
    "        schedule (str) : scheduling 방식\n",
    "        start, end (float) : beta 1, beta T 에 해당하는 값\n",
    "        n_timestep (int) : T \n",
    "    output : \n",
    "        betas (tensor (n_stepsize))\n",
    "    \"\"\" \n",
    "    if schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "    elif schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, n_timestep, dtype=torch.float64)\n",
    "    elif schedule == 'warmup10':\n",
    "        betas = _warmup_beta(start, end, n_timestep, 0.1)\n",
    "    elif schedule == 'warmup50':\n",
    "        betas = _warmup_beta(start, end, n_timestep, 0.5)\n",
    "    elif schedule == 'const':\n",
    "        betas = end * torch.ones(n_timestep, dtype=torch.float64)\n",
    "    elif schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        betas = 1. / (torch.linspace(n_timestep, 1, n_timestep, dtype=torch.float64))\n",
    "    else:\n",
    "        raise NotImplementedError(schedule)\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def _warmup_beta(start, end, n_timestep, warmup_frac):\n",
    "    \"\"\"\n",
    "    make_beta_schedule에서 schedule이 warmup10, warmup50 일 때 사용되는 함수\n",
    "    warmup_time만큼 증가, 그 이후로는 end 값으로 고정\n",
    "    input : \n",
    "        start, end (float) : beta 1, beta T 에 해당하는 값\n",
    "        n_timestep (int) : T\n",
    "        warmup_frac (float) : warmup_time의 비율\n",
    "    output : \n",
    "        betas (tensor (n_stepsize))\n",
    "    \"\"\"\n",
    "    betas               = end * torch.ones(n_timestep, dtype=torch.float64)\n",
    "    warmup_time         = int(n_timestep * warmup_frac)\n",
    "    betas[:warmup_time] = torch.linspace(start, end, warmup_time, dtype=torch.float64)\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    두 normal distribution의 mean, log-variance가 주어졌을 때 kl divergence를 계산하는 함수\n",
    "    input:\n",
    "        mean1, logvar1, mean2, logvar2 (float)\n",
    "    output:\n",
    "        kl-divergence (float)\n",
    "    \"\"\"\n",
    "\n",
    "    kl = 0.5 * (-1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2))\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "def extract(input, t, shape):\n",
    "    \"\"\"\n",
    "    input tensor에서 첫번째 차원을 기준으로 index가 t인 값 추출, output의 shape은 shape[0], 1,...,1\n",
    "    timestep에 맞는 값을 추출을 위해 사용. 예를 들어 input이 betas면, extract한 값은 beta_{t}\n",
    "    input:\n",
    "        input (tensor)\n",
    "        t (tensor) : batch 길이 tensor, 내부는 모두 time값으로 채워져있음. tensor([1,1,...1])\n",
    "        shape (tuple)\n",
    "    output:\n",
    "        out (tensor)\n",
    "    \"\"\"\n",
    "    out     = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [shape[0]] + [1] * (len(shape) - 1)\n",
    "    out     = out.reshape(*reshape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def noise_like(shape, noise_fn, repeat=False):\n",
    "    \"\"\"\n",
    "    입력한 repeat function에 따라 noise를 만들어주는 함수. \n",
    "    repeat이 false면 shape[0]을 첫번째 dim의 size로 하고, 나머지 dim의 size는 1인 noise tensor 생성. \n",
    "    repeat이 True면 shape의 dim을 가진 noise tensor 생성\n",
    "    input:\n",
    "        shape (tensor)\n",
    "        noise_fn (function)\n",
    "    output:\n",
    "        shape에 맞는 noise tensor (tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    if repeat:\n",
    "        resid = [1] * (len(shape) - 1)\n",
    "        shape_one = (1, *shape[1:])\n",
    "\n",
    "        return noise_fn(*shape_one).repeat(shape[0], *resid)\n",
    "\n",
    "    else:\n",
    "        return noise_fn(*shape)\n",
    "\n",
    "\n",
    "def approx_standard_normal_cdf(x):\n",
    "    \"\"\"\n",
    "    standard normal 의 CDF(x)의 approximation\n",
    "    input:\n",
    "        x (float)\n",
    "    output:\n",
    "        approx. of integral -infty to x of standard normal (float)\n",
    "    \"\"\"\n",
    "    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def discretized_gaussian_log_likelihood(x, *, means, log_scales):\n",
    "    \"\"\"\n",
    "    input x는 [0,255]의 int로 scaling 되어있고, 이를 [-1,1]로 rescale한 후 discretized된 log likelihood 계산하는 함수\n",
    "    논문 3.3 Data scaling~ 부분 참조\n",
    "    input:\n",
    "        x (tensor) : x_0, generated image\n",
    "        means (tensor) : mean of model at t=1\n",
    "        log_scales (tensor) : 0.5*log_variance of model at t=1  \n",
    "    output:\n",
    "        log_probs (tensor) : 각 xi 의 log_probability를 원소로 하는 tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Assumes data is integers [0, 255] rescaled to [-1, 1]\n",
    "    centered_x = x - means\n",
    "    inv_stdv   = torch.exp(-log_scales)\n",
    "    plus_in    = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus   = approx_standard_normal_cdf(plus_in)\n",
    "    min_in     = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min    = approx_standard_normal_cdf(min_in)\n",
    "\n",
    "    log_cdf_plus          = torch.log(torch.clamp(cdf_plus, min=1e-12))\n",
    "    log_one_minus_cdf_min = torch.log(torch.clamp(1 - cdf_min, min=1e-12))\n",
    "    cdf_delta             = cdf_plus - cdf_min\n",
    "    log_probs             = torch.where(x < -0.999, log_cdf_plus,\n",
    "                                        torch.where(x > 0.999, log_one_minus_cdf_min,\n",
    "                                                    torch.log(torch.clamp(cdf_delta, min=1e-12))))\n",
    "\n",
    "    return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e6ebc2-71ef-46f4-ae88-369dde250388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, betas, model_mean_type, model_var_type, loss_type):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            betas (tensor) : 미리 정해진 forward process 의 variance schedule, 1- (size = (n_timesteps) )\n",
    "            model_mean_type (str) : 논문 상에서는 eps 사용\n",
    "            model_var_type (str) : fixedsmall, fixedlarge가 각각 논문 3.2장에서 나온 두 종류의 variance. \n",
    "            loss_type (str) : 논문 상에서는 kl 사용\n",
    "        class 변수:\n",
    "            self.register로 만들어진 변수들 : betas를 통해 계산할 수 있는 값들. 추후 likelihood 계산에 바로 사용하기 위해 미리 계산\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        betas              = betas.type(torch.float64)\n",
    "        timesteps          = betas.shape[0]\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        self.model_mean_type = model_mean_type  # xprev, xstart, eps\n",
    "        self.model_var_type  = model_var_type   # learned, fixedsmall, fixedlarge\n",
    "        self.loss_type       = loss_type        # kl, mse\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, 0)\n",
    "        alphas_cumprod_prev = torch.cat(\n",
    "            (torch.tensor([1], dtype=torch.float64), alphas_cumprod[:-1]), 0\n",
    "        )\n",
    "        posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "\n",
    "        self.register(\"betas\", betas)\n",
    "        self.register(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n",
    "\n",
    "        self.register(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1 - alphas_cumprod))\n",
    "        self.register(\"log_one_minus_alphas_cumprod\", torch.log(1 - alphas_cumprod))\n",
    "        self.register(\"sqrt_recip_alphas_cumprod\", torch.rsqrt(alphas_cumprod))\n",
    "        self.register(\"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1 / alphas_cumprod - 1))\n",
    "        self.register(\"posterior_variance\", posterior_variance)\n",
    "        self.register(\"posterior_log_variance_clipped\",\n",
    "                      torch.log(torch.cat((posterior_variance[1].view(1, 1),\n",
    "                                           posterior_variance[1:].view(-1, 1)), 0)).view(-1))\n",
    "        self.register(\"posterior_mean_coef1\", (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)))\n",
    "        self.register(\"posterior_mean_coef2\", ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)))\n",
    "\n",
    "    def register(self, name, tensor):\n",
    "        \"\"\"\n",
    "        class 변수 등록을 위한 함수, class 선언 시에 변수로 설정된다.\n",
    "        input:\n",
    "            name (str) : 등록할 이름\n",
    "            tensor (tensor) : 등록할 tensor\n",
    "        \"\"\"\n",
    "        self.register_buffer(name, tensor.type(torch.float32))\n",
    "\n",
    "        \n",
    "    # === forward process ===\n",
    "    \n",
    "    \n",
    "    def q_mean_variance(self, x_0, t):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0)의 mean, variance, log_variance를 return하는 함수, 논문 (4)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch (cifar10 기준 size (32,3,32,32))\n",
    "        output:\n",
    "            mean, variance, log_variance : t 시점의 q의 mean, variance, log-variance (size는 x_0와 같음)\n",
    "        \"\"\"\n",
    "        mean = extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "        variance = extract(1. - self.alphas_cumprod, t, x_0.shape)\n",
    "        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0) sampling 하는 함수, 논문 (4)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch \n",
    "        output:\n",
    "            t 시점의 q(x_t|x_0)을 sampling 한 tensor \n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        return (extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "                + extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape) * noise)\n",
    "    \n",
    "    def q_sample_loop(self, x_0, T, device, freq=50):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0)들의 list를 return\n",
    "        t는 0~T 까지 return할 수 있으며 freq에 따라 return list의 size가 변경된다.\n",
    "        input:\n",
    "            freq (int): freq step마다 imglist에 append해준다. 예를 들어 freq가 50이면 x_0, x_50, x_100...을 imglist에 넣어준다\n",
    "        output:\n",
    "            imglist (list of tensor)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        imglist = [x_0]\n",
    "\n",
    "        clip = lambda x_: (x_.clamp(min=-1, max=1) )\n",
    "        for i in range(T+1):\n",
    "            if (i+1)%freq == 0:\n",
    "                imglist.append(clip(self.q_sample(x_0.to(device), torch.full((128,) , i).to(device) , noise.to(device))))\n",
    "        return imglist\n",
    "    \n",
    "    def q_posterior_mean_variance(self, x_0, x_t, t):\n",
    "        \"\"\"\n",
    "        q(x_(t-1)|x_t,x_0) 의 mean, variance, log_variance return하는 함수, 논문 (6), (7)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch \n",
    "            x_t (tensor): forward process를 timestep t만큼 했을 때\n",
    "        output:\n",
    "            mean, var, log_var_clipped : mean, variance, log-variance of q(x_(t-1)|x_t,x_0) \n",
    "        \"\"\"\n",
    "        mean            = (extract(self.posterior_mean_coef1, t, x_t.shape) * x_0\n",
    "                           + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t)\n",
    "        var             = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "\n",
    "        return mean, var, log_var_clipped\n",
    "\n",
    "    \n",
    "    # === reverse process ===\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"\n",
    "        mean type이 eps인 경우 x_0 예측, (12)식 변형\n",
    "        \"\"\"\n",
    "        \n",
    "        x_t = x_t.view(noise.shape)\n",
    "        \n",
    "        \n",
    "        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "                - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise)\n",
    "\n",
    "    def predict_start_from_prev(self, x_t, t, x_prev):\n",
    "        \"\"\"\n",
    "        mean type이 xprev인 경우 x_0 예측, (7)식 변형    \n",
    "        \"\"\"\n",
    "\n",
    "        return (extract(1./self.posterior_mean_coef1, t, x_t.shape) * x_prev -\n",
    "                extract(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape) * x_t)\n",
    "    \n",
    "    def p_mean_variance(self, model, x, t, clip_denoised, return_pred_x0, condition):\n",
    "        \"\"\"\n",
    "        reverse step model의 mean, var type에 따라 다른 mean, variance, log_variance를 return하는 함수.\n",
    "        var type이 fixedsmall, fixedlarge가 각각 논문 3.2장에서 나온 두 종류의 variance. \n",
    "        mean type이 eps인 경우가 논문의 (12)식 변형, xprev인 경우가 논문의 (7)식 변경\n",
    "        input:\n",
    "            model (class) : 본 논문에서는 Unet model\n",
    "            cleap_denoised (bool) : clipping 여부 결정 \n",
    "            return_pred_x0 (bool) : 예측한 x0 return 여부 결정 \n",
    "            \n",
    "        output:\n",
    "            mean, var, log_var, pred_x0 : mean, variance, log_variance, predicted x0 of reverse process\n",
    "        \"\"\"\n",
    "\n",
    "        model_output = model(x, condition, t)\n",
    "\n",
    "        # Learned or fixed variance?\n",
    "        if self.model_var_type == 'learned':\n",
    "            model_output, log_var = torch.split(model_output, 2, dim=-1)\n",
    "            var                   = torch.exp(log_var)\n",
    "\n",
    "        elif self.model_var_type in ['fixedsmall', 'fixedlarge']:\n",
    "\n",
    "            # below: only log_variance is used in the KL computations\n",
    "            var, log_var = {\n",
    "                # for 'fixedlarge', we set the initial (log-)variance like so to get a better decoder log likelihood\n",
    "                'fixedlarge': (self.betas, torch.log(torch.cat((self.posterior_variance[1].view(1, 1),\n",
    "                                                                self.betas[1:].view(-1, 1)), 0)).view(-1)),\n",
    "                'fixedsmall': (self.posterior_variance, self.posterior_log_variance_clipped),\n",
    "            }[self.model_var_type]\n",
    "\n",
    "            var     = extract(var, t, x.shape) * torch.ones_like(x)\n",
    "            log_var = extract(log_var, t, x.shape) * torch.ones_like(x)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_var_type)\n",
    "\n",
    "        # Mean parameterization\n",
    "        _maybe_clip = lambda x_: (x_.clamp(min=-1, max=1) if clip_denoised else x_)\n",
    "\n",
    "        if self.model_mean_type == 'xprev':\n",
    "            # the model predicts x_{t-1}\n",
    "            pred_x_0 = _maybe_clip(self.predict_start_from_prev(x_t=x, t=t, x_prev=model_output))\n",
    "            mean     = model_output\n",
    "        elif self.model_mean_type == 'xstart':\n",
    "            # the model predicts x_0\n",
    "            pred_x0    = _maybe_clip(model_output)\n",
    "            mean, _, _ = self.q_posterior_mean_variance(x_0=pred_x0, x_t=x, t=t)\n",
    "        elif self.model_mean_type == 'eps':\n",
    "            # the model predicts epsilon\n",
    "            pred_x0    = _maybe_clip(self.predict_start_from_noise(x_t=x, t=t, noise=model_output))\n",
    "            mean, _, _ = self.q_posterior_mean_variance(x_0=pred_x0, x_t=x, t=t)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_mean_type)\n",
    "\n",
    "        if return_pred_x0:\n",
    "            return mean, var, log_var, pred_x0\n",
    "        else:\n",
    "            return mean, var, log_var\n",
    "\n",
    "\n",
    "\n",
    "    def p_sample(self, model, x, t, noise_fn, clip_denoised=True, return_pred_x0=False, condition = torch.zeros(19)):\n",
    "        \"\"\"\n",
    "        t가 T...1일 때 mean, log_variance 이용해 x_(t-1) sampling, 논문의 Algorithm 2 참조\n",
    "        input:\n",
    "            noise_fn : 논문 상에서는 z로 표기\n",
    "        output:\n",
    "            sample : x_(t-1)\n",
    "        \"\"\"\n",
    "\n",
    "        mean, _, log_var, pred_x0 = self.p_mean_variance(model, x, t, clip_denoised, return_pred_x0=return_pred_x0, condition=condition)\n",
    "        noise                     = noise_fn(x.shape, dtype=x.dtype).to(x.device)\n",
    "\n",
    "        shape        = [x.shape[0]] + [1] * (x.ndim - 1)\n",
    "        nonzero_mask = (1 - (t == 0).type(torch.float32)).view(*shape).to(x.device)\n",
    "        sample       = mean + nonzero_mask * torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "        return sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, noise_fn=torch.randn, condition = torch.zeros(18)):\n",
    "        \"\"\"\n",
    "        t=T부터 0까지 이전 생성 image 이용해 sampling. t=0때 sampling 완료 후 image tensor return한다. 논문의 Algorithm 2 구현\n",
    "        input:\n",
    "            shape (tuple) : image tensor size \n",
    "        output:\n",
    "            img (tensor) : x_0에 해당하는 image tensor \n",
    "        \"\"\"\n",
    "\n",
    "        device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "        img    = noise_fn(shape).to(device)\n",
    "        condition = condition.to(device)\n",
    "        for i in reversed(range(self.num_timesteps)):\n",
    "            if (i+1) % 200 == 0 :\n",
    "                print(i)\n",
    "            img = self.p_sample(\n",
    "                model,\n",
    "                img,\n",
    "                torch.full((shape[0],), i, dtype=torch.int64).to(device),\n",
    "                noise_fn=noise_fn,\n",
    "                return_pred_x0=True,\n",
    "                condition = condition\n",
    "            )\n",
    "            \n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop_progressive(self, model, shape, device, noise_fn=torch.randn, include_x0_pred_freq=50, ret_type=\"img\", condition= torch.zeros(18)):\n",
    "        \"\"\"\n",
    "        p_sample_loop와 유사. \n",
    "        input:\n",
    "            include_x0_pred_freq (int) : include_x0_pred_freq 번 마다 그 시점의 x_0 예측값을 기록\n",
    "            ret_type (string): img 면 image 하나를 return, list면 generation process의 모든 timestep의 image list를 return. \n",
    "        output:\n",
    "            x0_preds_ (tensor) : (B, num_recorded_x0_pred, C, H, W) size의 tensor\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        img = noise_fn(shape, dtype=torch.float32).to(device)\n",
    "\n",
    "        num_recorded_x0_pred = self.num_timesteps // include_x0_pred_freq\n",
    "        x0_preds_            = torch.zeros((shape[0], num_recorded_x0_pred, *shape[1:]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        img = img.view(shape[0],-1)\n",
    "        clip = lambda x_: (x_.clamp(min=-1, max=1))\n",
    "        imglist = [clip(img)]\n",
    "        predx0_list = []\n",
    "        for i in reversed(range(self.num_timesteps)):\n",
    "\n",
    "            # Sample p(x_{t-1} | x_t) as usual\n",
    "            img, pred_x0 = self.p_sample(model=model,\n",
    "                                         x=img,\n",
    "                                         t=torch.full((shape[0],), i, dtype=torch.int64).to(device),\n",
    "                                         noise_fn=noise_fn,\n",
    "                                         return_pred_x0=True,\n",
    "                                         condition = condition)\n",
    "            imglist.append(clip(img))\n",
    "            img = img.view(shape[0],-1)\n",
    "            \n",
    "            # Keep track of prediction of x0\n",
    "            insert_mask = np.floor(i // include_x0_pred_freq) == torch.arange(num_recorded_x0_pred,\n",
    "                                                                              dtype=torch.int32,\n",
    "                                                                              device=device)\n",
    "\n",
    "            insert_mask = insert_mask.to(torch.float32).view(1, num_recorded_x0_pred, *([1] * len(shape[1:])))\n",
    "            x0_preds_   = insert_mask * pred_x0[:, None, ...] + (1. - insert_mask) * x0_preds_\n",
    "            if i%include_x0_pred_freq == 0:\n",
    "                predx0_list.append(pred_x0)\n",
    "                print(i)\n",
    "        if ret_type == \"list\":\n",
    "            return imglist, x0_preds_, predx0_list\n",
    "        elif ret_type == \"img\":\n",
    "            return img, x0_preds_\n",
    "\n",
    "    # === Log likelihood calculation ===\n",
    "\n",
    "    def _vb_terms_bpd(self, model, x_0, x_t, t, clip_denoised, return_pred_x0, condition):\n",
    "        \"\"\"\n",
    "        논문에서 사용한 loss를 구하는 함수. (5)식 참조\n",
    "        t가 0인 image는 decoder의 negative log likelihood 값 return\n",
    "        t가 0이 아닌 image는 KL-Divergence return    \n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = t.shape[0]\n",
    "        true_mean, _, true_log_variance_clipped    = self.q_posterior_mean_variance(x_0=x_0,\n",
    "                                                                                    x_t=x_t,\n",
    "                                                                                    t=t)\n",
    "        model_mean, _, model_log_variance, pred_x0 = self.p_mean_variance(model,\n",
    "                                                                          x=x_t,\n",
    "                                                                          t=t,\n",
    "                                                                          clip_denoised=clip_denoised,\n",
    "                                                                          return_pred_x0=True,\n",
    "                                                                         condition = condition)\n",
    "\n",
    "        kl = normal_kl(true_mean, true_log_variance_clipped, model_mean, model_log_variance)\n",
    "        kl = torch.mean(kl.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(x_0, means=model_mean, log_scales=0.5 * model_log_variance)\n",
    "        decoder_nll = torch.mean(decoder_nll.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "\n",
    "        # At the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "        output = torch.where(t == 0, decoder_nll, kl)\n",
    "\n",
    "        return (output, pred_x0) if return_pred_x0 else output\n",
    "\n",
    "    def training_losses(self, model, x_0, cond, t, noise=None):\n",
    "        \"\"\"\n",
    "        loss type이 kl인 경우 위의 _vb_terms_bpd 값 return\n",
    "        mse인 경우 mse loss return    \n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "\n",
    "        x_t = self.q_sample(x_0=x_0, t=t, noise=noise)\n",
    "\n",
    "        \n",
    "        # Calculate the loss\n",
    "        if self.loss_type == 'kl':\n",
    "            # the variational bound\n",
    "            losses = self._vb_terms_bpd(model=model, x_0=x_0, x_t=x_t, t=t, clip_denoised=False, return_pred_x0=False)\n",
    "\n",
    "        elif self.loss_type == 'mse':\n",
    "            # unweighted MSE\n",
    "            assert self.model_var_type != 'learned'\n",
    "            target = {\n",
    "                'xprev': self.q_posterior_mean_variance(x_0=x_0, x_t=x_t, t=t)[0],\n",
    "                'xstart': x_0,\n",
    "                'eps': noise\n",
    "            }[self.model_mean_type]\n",
    "\n",
    "            model_output = model(x_t, cond, t)\n",
    "            losses = torch.mean((target - model_output.view(model_output.shape[0], -1)).view(x_0.shape[0], -1)**2, dim=1)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _prior_bpd(self, x_0):\n",
    "        \"\"\"\n",
    "        x_0의 prior distribution에 대한 복원 오차 계산하는 함수\n",
    "        \"\"\"\n",
    "\n",
    "        B, T                        = x_0.shape[0], self.num_timesteps\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_0,\n",
    "                                                           t=torch.full((B,), T - 1, dtype=torch.int64))\n",
    "        kl_prior                    = normal_kl(mean1=qt_mean,\n",
    "                                                logvar1=qt_log_variance,\n",
    "                                                mean2=torch.zeros_like(qt_mean),\n",
    "                                                logvar2=torch.zeros_like(qt_log_variance))\n",
    "\n",
    "        return torch.mean(kl_prior.view(B, -1), dim=1)/np.log(2.)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calc_bpd_loop(self, model, x_0, clip_denoised, condition):\n",
    "        \"\"\"\n",
    "        모든 timestep 에 대한 prior distribution에 대한 복원 오차 계산하는 함수    \n",
    "        \"\"\"\n",
    "\n",
    "        (B, C, H, W), T = x_0.shape, self.num_timesteps\n",
    "\n",
    "        new_vals_bt = torch.zeros((B, T))\n",
    "        new_mse_bt  = torch.zeros((B, T))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "\n",
    "            t_b = torch.full((B, ), t, dtype=torch.int64)\n",
    "\n",
    "            # Calculate VLB term at the current timestep\n",
    "            new_vals_b, pred_x0 = self._vb_terms_bpd(model=model,\n",
    "                                                     x_0=x_0,\n",
    "                                                     x_t=self.q_sample(x_0=x_0, t=t_b),\n",
    "                                                     t=t_b,\n",
    "                                                     clip_denoised=clip_denoised,\n",
    "                                                     return_pred_x0=True,\n",
    "                                                     condition = condition)\n",
    "\n",
    "            # MSE for progressive prediction loss\n",
    "            new_mse_b = torch.mean((pred_x0-x_0).view(B, -1)**2, dim=1)\n",
    "\n",
    "            # Insert the calculated term into the tensor of all terms\n",
    "            mask_bt = (t_b[:, None] == torch.arange(T)[None, :]).to(torch.float32)\n",
    "\n",
    "            new_vals_bt = new_vals_bt * (1. - mask_bt) + new_vals_b[:, None] * mask_bt\n",
    "            new_mse_bt  = new_mse_bt  * (1. - mask_bt) + new_mse_b[:, None] * mask_bt\n",
    "\n",
    "        prior_bpd_b = self._prior_bpd(x_0)\n",
    "        total_bpd_b = torch.sum(new_vals_bt, dim=1) + prior_bpd_b\n",
    "\n",
    "        return total_bpd_b, new_vals_bt, prior_bpd_b, new_mse_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5735538-c5d1-40bd-b828-c48e2f7868d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adopted from https://github.com/rosinality/denoising-diffusion-pytorch with some minor changes.\n",
    "\n",
    "def swish(input):\n",
    "    \"\"\"\n",
    "    activation function. return x*sigmoid(x) \n",
    "    \"\"\"\n",
    "    return input * torch.sigmoid(input)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def variance_scaling_init_(tensor, scale=1, mode=\"fan_avg\", distribution=\"uniform\"):\n",
    "    \"\"\"\n",
    "    mode에 따라 scale이 바뀌고, 그 scale대로 weight tensor를 initialize 해준다.\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n",
    "\n",
    "    if mode == \"fan_in\":\n",
    "        scale /= fan_in\n",
    "\n",
    "    elif mode == \"fan_out\":\n",
    "        scale /= fan_out\n",
    "\n",
    "    else:\n",
    "        scale /= (fan_in + fan_out) / 2\n",
    "\n",
    "    if distribution == \"normal\":\n",
    "        std = math.sqrt(scale)\n",
    "\n",
    "        return tensor.normal_(0, std)\n",
    "\n",
    "    else:\n",
    "        bound = math.sqrt(3 * scale)\n",
    "\n",
    "        return tensor.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    in_channel,\n",
    "    out_channel,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    bias=True,\n",
    "    scale=1,\n",
    "    mode=\"fan_avg\",\n",
    "):\n",
    "    \"\"\"\n",
    "    convolution layer return하는 함수. \n",
    "    입력한 in_channel, out_channel, kernel_size, stride, padding 에 따라 convolution layer 만든다.\n",
    "    scale, mode에 따라 weight initialize 해주고 bias가 True면 0 bias를 가지도록 return.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv2d(\n",
    "        in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias\n",
    "    )\n",
    "\n",
    "    variance_scaling_init_(conv.weight, scale, mode=mode)\n",
    "\n",
    "    if bias:\n",
    "        nn.init.zeros_(conv.bias)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def linear(in_channel, out_channel, scale=1, mode=\"fan_avg\"):\n",
    "    \"\"\"\n",
    "    linear layer return 하는 함수.\n",
    "    in_channel, out_channel 에 따라 linear layer 만든다\n",
    "    scale, mode에 따라 weight initialize 해주고, 0 bias를 가지도록 해준다.\n",
    "    \"\"\"\n",
    "    lin = nn.Linear(in_channel, out_channel)\n",
    "\n",
    "    variance_scaling_init_(lin.weight, scale, mode=mode)\n",
    "    nn.init.zeros_(lin.bias)\n",
    "\n",
    "    return lin\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    위에서 정의한 swish 함수를 layer 형태로 만든 것\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return swish(input)\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Upsample과 conv2d를 layer로 가지는 nn.Sequential layer\n",
    "    channel을 scale factor만큼 키워준다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        layers = [\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            conv2d(channel, channel, 3, padding=1),\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Downsample(nn.Sequential):\n",
    "    \"\"\"\n",
    "    conv2d를 layer로 가지는 nn.Sequential layer. stride가 2라 output image의 크기가 줄어든다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        layers = [conv2d(channel, channel, 3, stride=2, padding=1)]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip connection을 사용한 network. time embedding 과 dropout을 hyperparameter로 가진다.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, time_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(32, in_channel)\n",
    "        self.activation1 = Swish()\n",
    "        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n",
    "\n",
    "        self.time = nn.Sequential(Swish(), linear(time_dim, out_channel))\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(32, out_channel)\n",
    "        self.activation2 = Swish()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n",
    "\n",
    "        if in_channel != out_channel:\n",
    "            self.skip = conv2d(in_channel, out_channel, 1)\n",
    "\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, input, time):\n",
    "        batch = input.shape[0]\n",
    "\n",
    "        out = self.conv1(self.activation1(self.norm1(input)))\n",
    "\n",
    "        out = out + self.time(time).view(batch, -1, 1, 1)\n",
    "\n",
    "        out = self.conv2(self.dropout(self.activation2(self.norm2(out))))\n",
    "\n",
    "        if self.skip is not None:\n",
    "            input = self.skip(input)\n",
    "\n",
    "        return out + input\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention 적용하는 layer\n",
    "    input의 channel 수 4배로 늘린 뒤 torch.chunk 로 q, k, v 나눔. \n",
    "    이때 channel의 수가 3의 배수 아닌데, query와 key의 channel수가 같고, value의 channel 수가 작음\n",
    "    나머지 계산의 dimension 계산은 아래 torch.einsum에 설명\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.GroupNorm(32, in_channel)\n",
    "        self.qkv = conv2d(in_channel, in_channel * 4, 1)\n",
    "        self.out = conv2d(in_channel, in_channel, 1, scale=1e-10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch, channel, height, width = input.shape\n",
    "\n",
    "        norm = self.norm(input)\n",
    "        qkv = self.qkv(norm)\n",
    "        query, key, value = qkv.chunk(3, dim=1)\n",
    "\n",
    "        attn = torch.einsum(\"nchw, ncyx -> nhwyx\", query, key).contiguous() / math.sqrt(\n",
    "            channel\n",
    "        )\n",
    "        attn = attn.view(batch, height, width, -1)\n",
    "        attn = torch.softmax(attn, -1)\n",
    "        attn = attn.view(batch, height, width, height, width)\n",
    "\n",
    "        out = torch.einsum(\"nhwyx, ncyx -> nchw\", attn, input).contiguous()\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out + input\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer의 sinusoidal positional embedding 적용\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim      = dim\n",
    "        half_dim      = self.dim // 2\n",
    "        self.inv_freq = torch.exp(torch.arange(half_dim, dtype=torch.float32) * (-math.log(10000) / (half_dim - 1)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        shape       = input.shape\n",
    "        input       = input.view(-1).to(torch.float32)\n",
    "        sinusoid_in = torch.ger(input, self.inv_freq.to(input.device))\n",
    "        pos_emb     = torch.cat([sinusoid_in.sin(), sinusoid_in.cos()], dim=-1)\n",
    "        pos_emb     = pos_emb.view(*shape, self.dim)\n",
    "        \n",
    "        return pos_emb\n",
    "\n",
    "\n",
    "class ResBlockWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet에서 기본단위로 사용되는 block\n",
    "    Resblock - SelfAttention으로 이루어짐\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, time_dim, dropout, use_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resblocks = ResBlock(in_channel, out_channel, time_dim, dropout)\n",
    "\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttention(out_channel)\n",
    "\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "    def forward(self, input, time):\n",
    "        out = self.resblocks(input, time)\n",
    "\n",
    "        if self.attention is not None:\n",
    "            out = self.attention(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def spatial_fold(input, fold):\n",
    "    \"\"\"\n",
    "    h, w 를 fold만큼 나눈 dim으로 바꿔주고, channel의 dimension을 그만큼 곱함\n",
    "    \"\"\"\n",
    "    if fold == 1:\n",
    "        return input\n",
    "\n",
    "    batch, channel, height, width = input.shape\n",
    "    h_fold = height // fold\n",
    "    w_fold = width // fold\n",
    "\n",
    "    return (\n",
    "        input.view(batch, channel, h_fold, fold, w_fold, fold)\n",
    "        .permute(0, 1, 3, 5, 2, 4)\n",
    "        .reshape(batch, -1, h_fold, w_fold)\n",
    "    )\n",
    "\n",
    "\n",
    "def spatial_unfold(input, unfold):\n",
    "    \"\"\"\n",
    "    spatial_fold와 반대.\n",
    "    h, w를 unfold만큼 곱한 dim으로 바꿔주고, channel의 dimension을 그만큼 나눔\n",
    "    \"\"\"\n",
    "    if unfold == 1:\n",
    "        return input\n",
    "\n",
    "    batch, channel, height, width = input.shape\n",
    "    h_unfold = height * unfold\n",
    "    w_unfold = width * unfold\n",
    "\n",
    "    return (\n",
    "        input.view(batch, -1, unfold, unfold, height, width)\n",
    "        .permute(0, 1, 4, 2, 5, 3)\n",
    "        .reshape(batch, -1, h_unfold, w_unfold)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    학습에 사용할 전체 모델.\n",
    "    dimension과 구조는 위의 markdown 그림 참조\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        channel,\n",
    "        channel_multiplier,\n",
    "        n_res_blocks,\n",
    "        attn_strides,\n",
    "        dropout=0,\n",
    "        fold=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fold = fold\n",
    "\n",
    "        time_dim = channel * 4\n",
    "        \n",
    "        self.enc_data = nn.Sequential(linear(18+numofcluster*100,3072))\n",
    "        \n",
    "        self.dec_data = nn.Sequential(linear(3072,numofcluster*100))\n",
    "\n",
    "        n_block = len(channel_multiplier)\n",
    "        \n",
    "\n",
    "        self.time = nn.Sequential(\n",
    "            TimeEmbedding(channel),\n",
    "            linear(channel, time_dim),\n",
    "            Swish(),\n",
    "            linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        down_layers   = [conv2d(in_channel * (fold ** 2), channel, 3, padding=1)]\n",
    "        feat_channels = [channel]\n",
    "        in_channel    = channel\n",
    "        for i in range(n_block):\n",
    "            for _ in range(n_res_blocks):\n",
    "                channel_mult = channel * channel_multiplier[i]\n",
    "\n",
    "                down_layers.append(\n",
    "                    ResBlockWithAttention(\n",
    "                        in_channel,\n",
    "                        channel_mult,\n",
    "                        time_dim,\n",
    "                        dropout,\n",
    "                        use_attention=2 ** i in attn_strides,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                feat_channels.append(channel_mult)\n",
    "                in_channel = channel_mult\n",
    "\n",
    "            if i != n_block - 1:\n",
    "                down_layers.append(Downsample(in_channel))\n",
    "                feat_channels.append(in_channel)\n",
    "\n",
    "        self.down = nn.ModuleList(down_layers)\n",
    "\n",
    "        self.mid = nn.ModuleList(\n",
    "            [\n",
    "                ResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    time_dim,\n",
    "                    dropout=dropout,\n",
    "                    use_attention=True,\n",
    "                ),\n",
    "                ResBlockWithAttention(\n",
    "                    in_channel, in_channel, time_dim, dropout=dropout\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        up_layers = []\n",
    "        for i in reversed(range(n_block)):\n",
    "            for _ in range(n_res_blocks + 1):\n",
    "                channel_mult = channel * channel_multiplier[i]\n",
    "\n",
    "                up_layers.append(\n",
    "                    ResBlockWithAttention(\n",
    "                        in_channel + feat_channels.pop(),\n",
    "                        channel_mult,\n",
    "                        time_dim,\n",
    "                        dropout=dropout,\n",
    "                        use_attention=2 ** i in attn_strides,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                in_channel = channel_mult\n",
    "\n",
    "            if i != 0:\n",
    "                up_layers.append(Upsample(in_channel))\n",
    "\n",
    "        self.up = nn.ModuleList(up_layers)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_channel),\n",
    "            Swish(),\n",
    "            conv2d(in_channel, 3 * (fold ** 2), 3, padding=1, scale=1e-10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, condition,time):\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        \n",
    "        x = torch.cat((x, condition),dim=1)\n",
    "        \n",
    "        x=self.enc_data(x)\n",
    "        time_embed = self.time(time)\n",
    "        feats = []\n",
    "        out = x.view(batch, 3,32,32)\n",
    "        \n",
    "        out = spatial_fold(out, self.fold)\n",
    "        \n",
    "        for layer in self.down:\n",
    "            if isinstance(layer, ResBlockWithAttention):\n",
    "                out = layer(out, time_embed)\n",
    "\n",
    "            else:\n",
    "                out = layer(out)\n",
    "\n",
    "            feats.append(out)\n",
    "\n",
    "        for layer in self.mid:\n",
    "            out = layer(out, time_embed)\n",
    "\n",
    "        for layer in self.up:\n",
    "            if isinstance(layer, ResBlockWithAttention):\n",
    "                out = layer(torch.cat((out, feats.pop()), 1), time_embed)\n",
    "\n",
    "            else:\n",
    "                out = layer(out)\n",
    "\n",
    "        out = self.out(out)\n",
    "        out = spatial_unfold(out, self.fold)\n",
    "        \n",
    "        out = out.view(batch, 3072)\n",
    "        kk = self.dec_data(out)\n",
    "        \n",
    "        return kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1876c762-a992-4c2f-a605-b1074b145ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[[5.  2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 2141)\n",
      "4.0\n",
      "[[2.5 4.  4.  ... 4.  2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 1457)\n",
      "1.0\n",
      "[[2.5 5.  2.5 ... 5.  4.  3. ]\n",
      " [2.5 5.  2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 3.  1.  2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 916)\n",
      "2.0\n",
      "[[2.5 2.5 2.5 ... 3.  2.5 2.5]\n",
      " [2.5 3.  2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 988)\n",
      "3.0\n",
      "[[3.  4.  2.5 ... 3.  4.  2.5]\n",
      " [2.5 3.  5.  ... 3.  2.5 2.5]\n",
      " [2.  2.5 2.5 ... 3.  2.  2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 538)\n",
      "0.0\n",
      "tensor([[0.5081, 0.5035, 0.5017,  ..., 0.5046, 0.5036, 0.5025],\n",
      "        [0.5048, 0.5043, 0.5020,  ..., 0.5034, 0.5020, 0.5016],\n",
      "        [0.5050, 0.5034, 0.5015,  ..., 0.5035, 0.5015, 0.5013],\n",
      "        ...,\n",
      "        [0.5046, 0.5044, 0.5022,  ..., 0.5034, 0.5016, 0.5013],\n",
      "        [0.5047, 0.5046, 0.5019,  ..., 0.5033, 0.5017, 0.5015],\n",
      "        [0.5049, 0.5043, 0.5027,  ..., 0.5040, 0.5014, 0.5016]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "tensor(0.4791, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5291, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "4.0\n",
      "tensor([[0.5703, 0.5176, 0.5779,  ..., 0.5657, 0.5490, 0.5447],\n",
      "        [0.5030, 0.5032, 0.5059,  ..., 0.5043, 0.5004, 0.5070],\n",
      "        [0.5020, 0.5038, 0.5051,  ..., 0.5025, 0.4971, 0.5067],\n",
      "        ...,\n",
      "        [0.5025, 0.5038, 0.5051,  ..., 0.5035, 0.4996, 0.5077],\n",
      "        [0.5025, 0.5042, 0.5050,  ..., 0.5040, 0.4999, 0.5073],\n",
      "        [0.5082, 0.5034, 0.5083,  ..., 0.5089, 0.5026, 0.5104]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "tensor(0.4794, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7398, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "1.0\n",
      "tensor([[0.5668, 0.7067, 0.6053,  ..., 0.5673, 0.5696, 0.6289],\n",
      "        [0.5102, 0.5494, 0.5150,  ..., 0.5170, 0.5085, 0.5331],\n",
      "        [0.4971, 0.5196, 0.5003,  ..., 0.5098, 0.4956, 0.5151],\n",
      "        ...,\n",
      "        [0.4997, 0.5256, 0.5018,  ..., 0.5103, 0.4976, 0.5176],\n",
      "        [0.4970, 0.5147, 0.4966,  ..., 0.5070, 0.4926, 0.5105],\n",
      "        [0.5073, 0.5398, 0.5099,  ..., 0.5138, 0.5021, 0.5264]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "tensor(0.4149, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8295, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "2.0\n",
      "tensor([[0.5422, 0.5755, 0.6505,  ..., 0.5593, 0.5848, 0.6055],\n",
      "        [0.5114, 0.5089, 0.5238,  ..., 0.5156, 0.5120, 0.5192],\n",
      "        [0.5068, 0.5010, 0.5077,  ..., 0.5098, 0.5012, 0.5039],\n",
      "        ...,\n",
      "        [0.5080, 0.5035, 0.5119,  ..., 0.5105, 0.5028, 0.5087],\n",
      "        [0.5068, 0.5013, 0.5089,  ..., 0.5102, 0.5017, 0.5062],\n",
      "        [0.5108, 0.5095, 0.5268,  ..., 0.5158, 0.5120, 0.5174]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "tensor(0.4515, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.8387, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "3.0\n",
      "tensor([[0.5101, 0.5567, 0.6410,  ..., 0.5481, 0.5731, 0.5896],\n",
      "        [0.5094, 0.5230, 0.5705,  ..., 0.5242, 0.5365, 0.5414],\n",
      "        [0.5089, 0.5219, 0.5608,  ..., 0.5231, 0.5362, 0.5334],\n",
      "        ...,\n",
      "        [0.5123, 0.5193, 0.5634,  ..., 0.5232, 0.5364, 0.5352],\n",
      "        [0.5114, 0.5156, 0.5553,  ..., 0.5206, 0.5318, 0.5298],\n",
      "        [0.5133, 0.5267, 0.5823,  ..., 0.5312, 0.5459, 0.5523]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "tensor(0.4423, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.7638, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing with multiple VAE\n",
    "\n",
    "new_data = pd.read_csv(filename1)\n",
    "if mode == \"sig\":\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, input_size, encoding_dim):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, input_size//3),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(input_size//3, encoding_dim)\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(encoding_dim, input_size//3),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(input_size//3, input_size),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "    \n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return encoded, decoded\n",
    "else:\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self, input_size, encoding_dim):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_size, input_size//3),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(input_size//3, encoding_dim)\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(encoding_dim, input_size//3),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(input_size//3, input_size),\n",
    "                \n",
    "            )\n",
    "    \n",
    "        def forward(self, x):\n",
    "            encoded = self.encoder(x)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return encoded, decoded\n",
    "\n",
    "df = new_data\n",
    "df_dict = {}\n",
    "df_encoded_dict = {}\n",
    "\n",
    "movieid = df.iloc[:-1,0]\n",
    "group_gmm_values = df.loc[df.index[-1], :]\n",
    "\n",
    "for group_value in group_gmm_values.unique():\n",
    "    group_df = df.loc[:, df.loc[df.index[-1], :] == group_value]\n",
    "    \n",
    "    df_dict[group_value] = group_df.iloc[:-1,:] # original data save\n",
    "    \n",
    "autoencoders = {}\n",
    "autoencoders_loaded = {}\n",
    "clusternum = {}\n",
    "for group_num in df_dict.keys():\n",
    "    if type(group_num)!=str:\n",
    "        print(group_num)\n",
    "        data = df_dict[group_num]\n",
    "        data = data.replace(np.nan, 2.5)\n",
    "        input_data = data.values\n",
    "        df_dict[group_num] = input_data\n",
    "        del(data)\n",
    "        gc.collect()\n",
    "        \n",
    "        # Convert the input data to a PyTorch tensor\n",
    "        print(input_data, input_data.shape)\n",
    "        clusternum[group_num] = input_data.shape[1]\n",
    "        input_tensor = torch.from_numpy(input_data).float().to(\"cuda\")\n",
    "\n",
    "        # Initialize the autoencoder model\n",
    "        input_size = input_data.shape[1] \n",
    "        encoding_dim = input_size  \n",
    "        autoencoder_new = Autoencoder(input_size, 100).to(\"cuda\")\n",
    "        if torch.cuda.device_count()>1:\n",
    "            autoencoder_new = nn.DataParallel(autoencoder_new)\n",
    "\n",
    "\n",
    "        ################# AE 이름 바꿔주기 #################\n",
    "        state_dict = torch.load(f'./trained_ckpt/{ckptname}_AE{group_num}.ckpt')\n",
    "        autoencoder_new.load_state_dict(state_dict)\n",
    "        autoencoder_new.eval()\n",
    "        autoencoders[group_num]=autoencoder_new\n",
    "\n",
    "        \n",
    "for group_num in df_dict.keys():\n",
    "    if type(group_num)!=str:\n",
    "        print(group_num)\n",
    "\n",
    "        if mode == \"sig\":\n",
    "            encoded, decoded = autoencoders[group_num](torch.from_numpy(df_dict[group_num]/5).float().to(\"cuda\"))\n",
    "            df_encoded_dict[group_num] = encoded\n",
    "        else:\n",
    "            encoded, decoded = autoencoders[group_num](torch.from_numpy(df_dict[group_num]).float().to(\"cuda\"))\n",
    "            df_encoded_dict[group_num] = encoded\n",
    "        decoded = torch.clamp(decoded,0,5)\n",
    "        print(decoded)\n",
    "        print(decoded.min(), decoded.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7220ad00-bd66-46f4-b5f2-452ca6ce6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3706, 500])\n",
      "torch.Size([3706, 18])\n",
      "middle =  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>) halfrange = tensor(1.9232, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_condition = \"./data/data_delete_nogenres_1m.csv\"\n",
    "cond_data = pd.read_csv(data_condition)\n",
    "\n",
    "for num, j in enumerate(movieid):\n",
    "    temp_cond = torch.tensor(cond_data.loc[cond_data[\"movieId\"]==int(j)].iloc[:,2:].values,dtype = torch.float32)\n",
    "    if temp_cond.shape[0]==0:\n",
    "        print(j)\n",
    "    if num==0:\n",
    "        condition = temp_cond\n",
    "        \n",
    "    else:\n",
    "        condition = torch.cat((condition, temp_cond), dim=0)\n",
    "        \n",
    "for i in df_encoded_dict.keys():\n",
    "    try:\n",
    "        input_data = torch.cat((input_data, df_encoded_dict[i]),dim=1)\n",
    "\n",
    "\n",
    "    except:\n",
    "        input_data = df_encoded_dict[i]\n",
    "\n",
    "print(input_data.shape)\n",
    "print(condition.shape)\n",
    "\n",
    "middle_input = (input_data.max()+input_data.min())/2\n",
    "halfrange = (input_data.max()-input_data.min()) / 2\n",
    "\n",
    "input_data = (input_data - middle_input) / halfrange\n",
    "\n",
    "print(\"middle = \", middle_input, \"halfrange =\", halfrange)\n",
    "\n",
    "\n",
    "Totaldata = (input_data, condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c34e87-81a2-4387-801a-57ff8c0c6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_data(conf, inputdata):\n",
    "    if conf.dataset.name == 'movielens':\n",
    "        data, condition = inputdata\n",
    "        data = data.to(\"cpu\").detach()\n",
    "        condition = condition.detach()\n",
    "        datatuple = (data, condition)\n",
    "        dataset = TensorDataset(*datatuple)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_size = 3000\n",
    "        train_set = Subset(dataset, range(train_size))\n",
    "        valid_set = Subset(dataset, range(train_size, data.shape[0]))\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25666abf-129b-406a-8fff-cd6ba5370875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## util 복붙\n",
    "\n",
    "class obj(object):\n",
    "    \"\"\"\n",
    "    config가 json 형식으로 되어있습니다.\n",
    "    json 파일의 데이터를 받아오는 형식을 바꿔주는 함수입니다. \n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "               setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.9999):\n",
    "    \"\"\"\n",
    "    아래 DDP class에서 EMA를 진행하기 위한 함수입니다.\n",
    "    model1의 parameter을 기존 param*decay + model2의 param*(1-decay)로 update 합니다.\n",
    "    \"\"\"\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def samples_fn(model, diffusion, shape, condition):\n",
    "    \"\"\"\n",
    "    sample image의 현재 범위는 -1~1입니다.\n",
    "    이를 image화 시키려먼 0~1 사이의 범위로 만들어주어야 하고, 이를 수행하는 함수입니다.\n",
    "    \"\"\"\n",
    "    samples = diffusion.p_sample_loop(model=model,\n",
    "                                      shape=shape,\n",
    "                                      noise_fn=torch.randn,\n",
    "                                     condition = condition)\n",
    "    return {\n",
    "      'samples': (samples + 1)/2\n",
    "    }\n",
    "\n",
    "\n",
    "def progressive_samples_fn(model, diffusion, shape, device, include_x0_pred_freq=50, ret_type = \"img\", condition= torch.zeros(19)):\n",
    "    \"\"\"\n",
    "    위 samples_fn과 역할이 동일합니다.\n",
    "    \"\"\"\n",
    "    samples, progressive_samples, pro_list = diffusion.p_sample_loop_progressive(\n",
    "        model=model,\n",
    "        shape=shape,\n",
    "        noise_fn=torch.randn,\n",
    "        device=device,\n",
    "        include_x0_pred_freq=include_x0_pred_freq,\n",
    "        ret_type = ret_type,\n",
    "        condition = condition\n",
    "    )\n",
    "    if ret_type == \"list\":\n",
    "        for i in range(len(samples)):\n",
    "            samples[i] = (samples[i] + 1)/2\n",
    "        for j in range(len(pro_list)):\n",
    "            pro_list[j] = (pro_list[j]+1)/2\n",
    "        return {'samples': samples, 'progressive_samples': (progressive_samples + 1)/2, \"pro_list\" : pro_list}\n",
    "    elif ret_type == \"img\":\n",
    "        samples = (samples +1)/2\n",
    "        return {'samples': samples, 'progressive_samples': (progressive_samples + 1)/2}\n",
    "\n",
    "\n",
    "def bpd_fn(model, diffusion, x, condition):\n",
    "    \"\"\"\n",
    "    복원 오차 계산하는 함수. diffusion.calc_bpd_loop 참고\n",
    "    \"\"\"\n",
    "    total_bpd_b, terms_bpd_bt, prior_bpd_b, mse_bt = diffusion.calc_bpd_loop(model=model, x_0=x, clip_denoised=True, condition = condition)\n",
    "\n",
    "    return {\n",
    "      'total_bpd': total_bpd_b,\n",
    "      'terms_bpd': terms_bpd_bt,\n",
    "      'prior_bpd': prior_bpd_b,\n",
    "      'mse': mse_bt\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(val_loader, model, diffusion, condition):\n",
    "    \"\"\"\n",
    "    validation dataset에 대해 bpd, mse 계산하는 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    bpd = []\n",
    "    mse = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(iter(val_loader)):\n",
    "            x       = x\n",
    "            metrics = bpd_fn(model, diffusion, x, condition)\n",
    "\n",
    "            bpd.append(metrics['total_bpd'].view(-1, 1))\n",
    "            mse.append(metrics['mse'].view(-1, 1))\n",
    "\n",
    "        bpd = torch.cat(bpd, dim=0).mean()\n",
    "        mse = torch.cat(mse, dim=0).mean()\n",
    "\n",
    "    return bpd, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2bb412-2881-49f3-8099-2e81d3c5a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DDP(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, conf, inputdata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conf  = conf\n",
    "        self.save_hyperparameters()\n",
    "        self.input = inputdata\n",
    "\n",
    "        self.model = UNet(self.conf.model.in_channel,\n",
    "                          self.conf.model.channel,\n",
    "                          channel_multiplier=self.conf.model.channel_multiplier,\n",
    "                          n_res_blocks=self.conf.model.n_res_blocks,\n",
    "                          attn_strides=self.conf.model.attn_strides,\n",
    "                          dropout=self.conf.model.dropout,\n",
    "                          fold=self.conf.model.fold\n",
    "                          )\n",
    "        self.ema   = UNet(self.conf.model.in_channel,\n",
    "                          self.conf.model.channel,\n",
    "                          channel_multiplier=self.conf.model.channel_multiplier,\n",
    "                          n_res_blocks=self.conf.model.n_res_blocks,\n",
    "                          attn_strides=self.conf.model.attn_strides,\n",
    "                          dropout=self.conf.model.dropout,\n",
    "                          fold=self.conf.model.fold\n",
    "                          )\n",
    "\n",
    "        self.betas = make_beta_schedule(schedule=self.conf.model.schedule.type,\n",
    "                                        start=self.conf.model.schedule.beta_start,\n",
    "                                        end=self.conf.model.schedule.beta_end,\n",
    "                                        n_timestep=self.conf.model.schedule.n_timestep)\n",
    "\n",
    "        self.diffusion = GaussianDiffusion(betas=self.betas,\n",
    "                                           model_mean_type=self.conf.model.mean_type,\n",
    "                                           model_var_type=self.conf.model.var_type,\n",
    "                                           loss_type=self.conf.model.loss_type)\n",
    "        \n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_set, self.valid_set = get_train_data(self.conf, self.input)\n",
    "        \n",
    "\n",
    "    def forward(self, x, condition):\n",
    "\n",
    "        return self.diffusion.p_sample_loop(self.model, x.shape, condition = condition)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        if self.conf.training.optimizer.type == 'adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.conf.training.optimizer.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "\n",
    "        img, cond = batch[0], batch[1]\n",
    "        time   = (torch.rand(img.shape[0]) * 1000).type(torch.int64).to(img.device)\n",
    "        loss   = self.diffusion.training_losses(self.model, img, cond, time).mean()\n",
    "\n",
    "        accumulate(self.ema, self.model.module if isinstance(self.model, nn.DataParallel) else self.model, 0.9999)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_loader = DataLoader(self.train_set,\n",
    "                                  batch_size=self.conf.training.dataloader.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=self.conf.training.dataloader.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=self.conf.training.dataloader.drop_last)\n",
    "        return train_loader\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        img, cond = batch[0], batch[1]\n",
    "        time   = (torch.rand(img.shape[0]) * 1000).type(torch.int64).to(img.device)\n",
    "        loss   = self.diffusion.training_losses(self.ema, img, cond, time).mean()\n",
    "\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss         = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "\n",
    "        \n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = DataLoader(self.valid_set,\n",
    "                                  batch_size=self.conf.validation.dataloader.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=self.conf.validation.dataloader.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=self.conf.validation.dataloader.drop_last)\n",
    "\n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaa1b77-1fcf-4d7e-b23d-3199ec06b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"train\": False,\n",
    "        \"config\": 'config/diffusion_movielens.json',\n",
    "        \"ckpt_dir\": './ckpts',\n",
    "        \"ckpt_freq\": 5,\n",
    "        \"n_gpu\": 2,\n",
    "        \"model_dir\": './ckpts/last.ckpt',\n",
    "        \"sample_dir\": 'samples',\n",
    "        \"prog_sample_freq\": 100,\n",
    "        \"n_samples\": 100\n",
    "    })\n",
    "\n",
    "    \n",
    "path_to_config = args.config\n",
    "with open(path_to_config, 'r') as f:\n",
    "    conf = json.load(f)\n",
    "\n",
    "conf = obj(conf)\n",
    "denoising_diffusion_model = DDP(conf, Totaldata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924536db-3adc-48f2-abf7-e2066484c8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDP(\n",
       "  (model): UNet(\n",
       "    (enc_data): Sequential(\n",
       "      (0): Linear(in_features=518, out_features=3072, bias=True)\n",
       "    )\n",
       "    (dec_data): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=500, bias=True)\n",
       "    )\n",
       "    (time): Sequential(\n",
       "      (0): TimeEmbedding()\n",
       "      (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (2): Swish()\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (down): ModuleList(\n",
       "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Downsample(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): Downsample(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (7): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): Downsample(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (10): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (11): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid): ModuleList(\n",
       "      (0): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attention): SelfAttention(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Upsample(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (4): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): Upsample(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (8): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (12): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (13): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (14): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (ema): UNet(\n",
       "    (enc_data): Sequential(\n",
       "      (0): Linear(in_features=518, out_features=3072, bias=True)\n",
       "    )\n",
       "    (dec_data): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=500, bias=True)\n",
       "    )\n",
       "    (time): Sequential(\n",
       "      (0): TimeEmbedding()\n",
       "      (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (2): Swish()\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (down): ModuleList(\n",
       "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Downsample(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): Downsample(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (7): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): Downsample(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (10): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (11): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid): ModuleList(\n",
       "      (0): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attention): SelfAttention(\n",
       "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (qkv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Upsample(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (4): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): Upsample(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (8): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (11): Upsample(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (12): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (13): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (14): ResBlockWithAttention(\n",
       "        (resblocks): ResBlock(\n",
       "          (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (activation1): Swish()\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time): Sequential(\n",
       "            (0): Swish()\n",
       "            (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (activation2): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (skip): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (diffusion): GaussianDiffusion()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoising_diffusion_model.cuda()\n",
    "\n",
    "state_dict = torch.load(f'./trained_ckpt/{ckptname}.ckpt')\n",
    "denoising_diffusion_model.load_state_dict(state_dict)\n",
    "denoising_diffusion_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7be60c-e267-4889-83f6-38d7bd1d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(condition, device, model, conf):\n",
    "    condition = condition.to(device)\n",
    "    \n",
    "    sample = samples_fn(model.ema,\n",
    "                    model.diffusion,\n",
    "                    (condition.shape[0], numofcluster*100),\n",
    "                    condition = condition)\n",
    "    data = sample[\"samples\"] \n",
    "    \n",
    "    # print(data)\n",
    "    # print(data.shape)\n",
    "    # print(data.min(), data.max())\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e84fa3-9dfd-481c-9b67-84a74946b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "data_condition = \"./data/data_delete_nogenres_1m.csv\"\n",
    "cond_data = pd.read_csv(data_condition)\n",
    "\n",
    "genre = pd.read_csv(data_condition)\n",
    "genre = genre.drop([\"title\"], axis=\"columns\")\n",
    "\n",
    "print(genre.values[:,1:].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25bc7df-dfbf-4589-874e-1e964526f052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 18])\n",
      "999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     condition \u001b[38;5;241m=\u001b[39m orgcondition[numbatch\u001b[38;5;241m*\u001b[39mi:]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(condition\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 14\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43msampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoising_diffusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m predict \u001b[38;5;241m=\u001b[39m predict\u001b[38;5;241m*\u001b[39mhalfrange\u001b[38;5;241m+\u001b[39mmiddle_input\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(autoencoders\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36msampling\u001b[1;34m(condition, device, model, conf)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampling\u001b[39m(condition, device, model, conf):\n\u001b[0;32m      2\u001b[0m     condition \u001b[38;5;241m=\u001b[39m condition\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43msamples_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumofcluster\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m] \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# print(data.shape)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# print(data.min(), data.max())\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36msamples_fn\u001b[1;34m(model, diffusion, shape, condition)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msamples_fn\u001b[39m(model, diffusion, shape, condition):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    sample image의 현재 범위는 -1~1입니다.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    이를 image화 시키려먼 0~1 사이의 범위로 만들어주어야 하고, 이를 수행하는 함수입니다.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mnoise_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     38\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m'\u001b[39m: (samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     39\u001b[0m     }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 234\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[1;34m(self, model, shape, noise_fn, condition)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m--> 234\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_pred_x0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "Cell \u001b[1;32mIn[3], line 210\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[1;34m(self, model, x, t, noise_fn, clip_denoised, return_pred_x0, condition)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03mt가 T...1일 때 mean, log_variance 이용해 x_(t-1) sampling, 논문의 Algorithm 2 참조\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03minput:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    sample : x_(t-1)\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m mean, _, log_var, pred_x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_mean_variance(model, x, t, clip_denoised, return_pred_x0\u001b[38;5;241m=\u001b[39mreturn_pred_x0, condition\u001b[38;5;241m=\u001b[39mcondition)\n\u001b[1;32m--> 210\u001b[0m noise                     \u001b[38;5;241m=\u001b[39m \u001b[43mnoise_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m shape        \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    213\u001b[0m nonzero_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "condition = genre.values[:,1:]\n",
    "orgcondition = torch.tensor(condition,dtype=torch.float32)\n",
    "savelist = []\n",
    "numcondbatch = 20\n",
    "numbatch = 3883 // numcondbatch\n",
    "\n",
    "for i in range(numcondbatch):\n",
    "    if i != numcondbatch - 1:\n",
    "        condition = orgcondition[numbatch*i:numbatch*(i+1)]\n",
    "    else:\n",
    "        condition = orgcondition[numbatch*i:]\n",
    "    print(condition.shape)\n",
    "        \n",
    "    predict = sampling(condition, \"cuda\", denoising_diffusion_model, conf )\n",
    "    \n",
    "    predict = predict*halfrange+middle_input\n",
    "    \n",
    "    for num, key in enumerate(autoencoders.keys()):\n",
    "        newtensor = autoencoders[key].module.decoder(predict[:,num*100:(num+1)*100])\n",
    "        if mode == \"sig\":\n",
    "            newtensor = newtensor * 5\n",
    "            \n",
    "        if num==0:\n",
    "            temp = newtensor\n",
    "        else:\n",
    "            temp = torch.cat((temp, newtensor),dim=1)\n",
    "    print(temp.shape)\n",
    "    savelist.append(temp)\n",
    "    \n",
    "cpu_tensor_list = [tensor.detach().cpu() for tensor in savelist]\n",
    "df = pd.DataFrame(torch.cat(cpu_tensor_list).numpy())\n",
    "df = df.applymap(lambda x: max(0, min(5, x)))\n",
    "df.set_index(keys=cond_data[\"movieId\"], inplace=True, drop=True)\n",
    "df.to_csv(f\"./{ckptname}_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5580b-c246-479c-adf6-ecade57561dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2918ffe-71d6-49d1-bf6b-4ee8a07dba88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c2dda-a38c-49e1-be2f-2d7b5c349d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdba58d-ee97-433a-bf2d-09e1638d0e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac8b11-34fd-41ff-a7ea-755c231b686f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
