{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44aae61-a343-4cb5-b330-feddb192bafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m pip install -r ./requirements_for_cuda111.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a251321-3315-4044-85f3-5e7d89d089b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import easydict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "############################### Change this term for your input dataset ###############################\n",
    "\n",
    "numofcluster = 5\n",
    "clustertype = \"1st\"\n",
    "\n",
    "path = \"./data/\"\n",
    "filename1 = path + f\"ml_1m_user_mov_{clustertype}_cluster={numofcluster}.csv\"\n",
    "new_data = pd.read_csv(filename1)\n",
    "\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec027f-ce1a-46f6-8d1d-26ac98c05d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__) # 1.9.0+cu1x1 ??\n",
    "print(pl.__version__) # 0.8.5 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f80e7-6ca9-4240-b942-34da06fc9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(schedule, start, end, n_timestep):\n",
    "    \"\"\"\n",
    "    입력한 scheduling 방식에 따라 forward process에서 사용되는 beta를 만들어주는 함수\n",
    "    input : \n",
    "        schedule (str) : scheduling 방식\n",
    "        start, end (float) : beta 1, beta T 에 해당하는 값\n",
    "        n_timestep (int) : T \n",
    "    output : \n",
    "        betas (tensor (n_stepsize))\n",
    "    \"\"\" \n",
    "    if schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "    elif schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, n_timestep, dtype=torch.float64)\n",
    "    elif schedule == 'warmup10':\n",
    "        betas = _warmup_beta(start, end, n_timestep, 0.1)\n",
    "    elif schedule == 'warmup50':\n",
    "        betas = _warmup_beta(start, end, n_timestep, 0.5)\n",
    "    elif schedule == 'const':\n",
    "        betas = end * torch.ones(n_timestep, dtype=torch.float64)\n",
    "    elif schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        betas = 1. / (torch.linspace(n_timestep, 1, n_timestep, dtype=torch.float64))\n",
    "    else:\n",
    "        raise NotImplementedError(schedule)\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def _warmup_beta(start, end, n_timestep, warmup_frac):\n",
    "    \"\"\"\n",
    "    make_beta_schedule에서 schedule이 warmup10, warmup50 일 때 사용되는 함수\n",
    "    warmup_time만큼 증가, 그 이후로는 end 값으로 고정\n",
    "    input : \n",
    "        start, end (float) : beta 1, beta T 에 해당하는 값\n",
    "        n_timestep (int) : T\n",
    "        warmup_frac (float) : warmup_time의 비율\n",
    "    output : \n",
    "        betas (tensor (n_stepsize))\n",
    "    \"\"\"\n",
    "    betas               = end * torch.ones(n_timestep, dtype=torch.float64)\n",
    "    warmup_time         = int(n_timestep * warmup_frac)\n",
    "    betas[:warmup_time] = torch.linspace(start, end, warmup_time, dtype=torch.float64)\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    두 normal distribution의 mean, log-variance가 주어졌을 때 kl divergence를 계산하는 함수\n",
    "    input:\n",
    "        mean1, logvar1, mean2, logvar2 (float)\n",
    "    output:\n",
    "        kl-divergence (float)\n",
    "    \"\"\"\n",
    "\n",
    "    kl = 0.5 * (-1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2))\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "def extract(input, t, shape):\n",
    "    \"\"\"\n",
    "    input tensor에서 첫번째 차원을 기준으로 index가 t인 값 추출, output의 shape은 shape[0], 1,...,1\n",
    "    timestep에 맞는 값을 추출을 위해 사용. 예를 들어 input이 betas면, extract한 값은 beta_{t}\n",
    "    input:\n",
    "        input (tensor)\n",
    "        t (tensor) : batch 길이 tensor, 내부는 모두 time값으로 채워져있음. tensor([1,1,...1])\n",
    "        shape (tuple)\n",
    "    output:\n",
    "        out (tensor)\n",
    "    \"\"\"\n",
    "    out     = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [shape[0]] + [1] * (len(shape) - 1)\n",
    "    out     = out.reshape(*reshape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def noise_like(shape, noise_fn, repeat=False):\n",
    "    \"\"\"\n",
    "    입력한 repeat function에 따라 noise를 만들어주는 함수. \n",
    "    repeat이 false면 shape[0]을 첫번째 dim의 size로 하고, 나머지 dim의 size는 1인 noise tensor 생성. \n",
    "    repeat이 True면 shape의 dim을 가진 noise tensor 생성\n",
    "    input:\n",
    "        shape (tensor)\n",
    "        noise_fn (function)\n",
    "    output:\n",
    "        shape에 맞는 noise tensor (tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    if repeat:\n",
    "        resid = [1] * (len(shape) - 1)\n",
    "        shape_one = (1, *shape[1:])\n",
    "\n",
    "        return noise_fn(*shape_one).repeat(shape[0], *resid)\n",
    "\n",
    "    else:\n",
    "        return noise_fn(*shape)\n",
    "\n",
    "\n",
    "def approx_standard_normal_cdf(x):\n",
    "    \"\"\"\n",
    "    standard normal 의 CDF(x)의 approximation\n",
    "    input:\n",
    "        x (float)\n",
    "    output:\n",
    "        approx. of integral -infty to x of standard normal (float)\n",
    "    \"\"\"\n",
    "    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def discretized_gaussian_log_likelihood(x, *, means, log_scales):\n",
    "    \"\"\"\n",
    "    input x는 [0,255]의 int로 scaling 되어있고, 이를 [-1,1]로 rescale한 후 discretized된 log likelihood 계산하는 함수\n",
    "    논문 3.3 Data scaling~ 부분 참조\n",
    "    input:\n",
    "        x (tensor) : x_0, generated image\n",
    "        means (tensor) : mean of model at t=1\n",
    "        log_scales (tensor) : 0.5*log_variance of model at t=1  \n",
    "    output:\n",
    "        log_probs (tensor) : 각 xi 의 log_probability를 원소로 하는 tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Assumes data is integers [0, 255] rescaled to [-1, 1]\n",
    "    centered_x = x - means\n",
    "    inv_stdv   = torch.exp(-log_scales)\n",
    "    plus_in    = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus   = approx_standard_normal_cdf(plus_in)\n",
    "    min_in     = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min    = approx_standard_normal_cdf(min_in)\n",
    "\n",
    "    log_cdf_plus          = torch.log(torch.clamp(cdf_plus, min=1e-12))\n",
    "    log_one_minus_cdf_min = torch.log(torch.clamp(1 - cdf_min, min=1e-12))\n",
    "    cdf_delta             = cdf_plus - cdf_min\n",
    "    log_probs             = torch.where(x < -0.999, log_cdf_plus,\n",
    "                                        torch.where(x > 0.999, log_one_minus_cdf_min,\n",
    "                                                    torch.log(torch.clamp(cdf_delta, min=1e-12))))\n",
    "\n",
    "    return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2df791-80df-45da-b2d7-cfd7bde6d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, betas, model_mean_type, model_var_type, loss_type):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            betas (tensor) : 미리 정해진 forward process 의 variance schedule, 1- (size = (n_timesteps) )\n",
    "            model_mean_type (str) : 논문 상에서는 eps 사용\n",
    "            model_var_type (str) : fixedsmall, fixedlarge가 각각 논문 3.2장에서 나온 두 종류의 variance. \n",
    "            loss_type (str) : 논문 상에서는 kl 사용\n",
    "        class 변수:\n",
    "            self.register로 만들어진 변수들 : betas를 통해 계산할 수 있는 값들. 추후 likelihood 계산에 바로 사용하기 위해 미리 계산\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        betas              = betas.type(torch.float64)\n",
    "        timesteps          = betas.shape[0]\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        self.model_mean_type = model_mean_type  # xprev, xstart, eps\n",
    "        self.model_var_type  = model_var_type   # learned, fixedsmall, fixedlarge\n",
    "        self.loss_type       = loss_type        # kl, mse\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, 0)\n",
    "        alphas_cumprod_prev = torch.cat(\n",
    "            (torch.tensor([1], dtype=torch.float64), alphas_cumprod[:-1]), 0\n",
    "        )\n",
    "        posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "\n",
    "        self.register(\"betas\", betas)\n",
    "        self.register(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n",
    "\n",
    "        self.register(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1 - alphas_cumprod))\n",
    "        self.register(\"log_one_minus_alphas_cumprod\", torch.log(1 - alphas_cumprod))\n",
    "        self.register(\"sqrt_recip_alphas_cumprod\", torch.rsqrt(alphas_cumprod))\n",
    "        self.register(\"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1 / alphas_cumprod - 1))\n",
    "        self.register(\"posterior_variance\", posterior_variance)\n",
    "        self.register(\"posterior_log_variance_clipped\",\n",
    "                      torch.log(torch.cat((posterior_variance[1].view(1, 1),\n",
    "                                           posterior_variance[1:].view(-1, 1)), 0)).view(-1))\n",
    "        self.register(\"posterior_mean_coef1\", (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)))\n",
    "        self.register(\"posterior_mean_coef2\", ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)))\n",
    "\n",
    "    def register(self, name, tensor):\n",
    "        \"\"\"\n",
    "        class 변수 등록을 위한 함수, class 선언 시에 변수로 설정된다.\n",
    "        input:\n",
    "            name (str) : 등록할 이름\n",
    "            tensor (tensor) : 등록할 tensor\n",
    "        \"\"\"\n",
    "        self.register_buffer(name, tensor.type(torch.float32))\n",
    "\n",
    "        \n",
    "    # === forward process ===\n",
    "    \n",
    "    \n",
    "    def q_mean_variance(self, x_0, t):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0)의 mean, variance, log_variance를 return하는 함수, 논문 (4)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch (cifar10 기준 size (32,3,32,32))\n",
    "        output:\n",
    "            mean, variance, log_variance : t 시점의 q의 mean, variance, log-variance (size는 x_0와 같음)\n",
    "        \"\"\"\n",
    "        mean = extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "        variance = extract(1. - self.alphas_cumprod, t, x_0.shape)\n",
    "        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0) sampling 하는 함수, 논문 (4)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch \n",
    "        output:\n",
    "            t 시점의 q(x_t|x_0)을 sampling 한 tensor \n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        return (extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "                + extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape) * noise)\n",
    "    \n",
    "    def q_sample_loop(self, x_0, T, device, freq=50):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0)들의 list를 return\n",
    "        t는 0~T 까지 return할 수 있으며 freq에 따라 return list의 size가 변경된다.\n",
    "        input:\n",
    "            freq (int): freq step마다 imglist에 append해준다. 예를 들어 freq가 50이면 x_0, x_50, x_100...을 imglist에 넣어준다\n",
    "        output:\n",
    "            imglist (list of tensor)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        imglist = [x_0]\n",
    "\n",
    "        clip = lambda x_: (x_.clamp(min=-1, max=1) )\n",
    "        for i in range(T+1):\n",
    "            if (i+1)%freq == 0:\n",
    "                imglist.append(clip(self.q_sample(x_0.to(device), torch.full((128,) , i).to(device) , noise.to(device))))\n",
    "        return imglist\n",
    "    \n",
    "    def q_posterior_mean_variance(self, x_0, x_t, t):\n",
    "        \"\"\"\n",
    "        q(x_(t-1)|x_t,x_0) 의 mean, variance, log_variance return하는 함수, 논문 (6), (7)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch \n",
    "            x_t (tensor): forward process를 timestep t만큼 했을 때\n",
    "        output:\n",
    "            mean, var, log_var_clipped : mean, variance, log-variance of q(x_(t-1)|x_t,x_0) \n",
    "        \"\"\"\n",
    "        mean            = (extract(self.posterior_mean_coef1, t, x_t.shape) * x_0\n",
    "                           + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t)\n",
    "        var             = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "\n",
    "        return mean, var, log_var_clipped\n",
    "\n",
    "    \n",
    "    # === reverse process ===\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"\n",
    "        mean type이 eps인 경우 x_0 예측, (12)식 변형\n",
    "        \"\"\"\n",
    "        \n",
    "        x_t = x_t.view(noise.shape)\n",
    "        \n",
    "        \n",
    "        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "                - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise)\n",
    "\n",
    "    def predict_start_from_prev(self, x_t, t, x_prev):\n",
    "        \"\"\"\n",
    "        mean type이 xprev인 경우 x_0 예측, (7)식 변형    \n",
    "        \"\"\"\n",
    "\n",
    "        return (extract(1./self.posterior_mean_coef1, t, x_t.shape) * x_prev -\n",
    "                extract(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape) * x_t)\n",
    "    \n",
    "    def p_mean_variance(self, model, x, t, clip_denoised, return_pred_x0, condition):\n",
    "        \"\"\"\n",
    "        reverse step model의 mean, var type에 따라 다른 mean, variance, log_variance를 return하는 함수.\n",
    "        var type이 fixedsmall, fixedlarge가 각각 논문 3.2장에서 나온 두 종류의 variance. \n",
    "        mean type이 eps인 경우가 논문의 (12)식 변형, xprev인 경우가 논문의 (7)식 변경\n",
    "        input:\n",
    "            model (class) : 본 논문에서는 Unet model\n",
    "            cleap_denoised (bool) : clipping 여부 결정 \n",
    "            return_pred_x0 (bool) : 예측한 x0 return 여부 결정 \n",
    "            \n",
    "        output:\n",
    "            mean, var, log_var, pred_x0 : mean, variance, log_variance, predicted x0 of reverse process\n",
    "        \"\"\"\n",
    "\n",
    "        model_output = model(x, condition, t)\n",
    "\n",
    "        # Learned or fixed variance?\n",
    "        if self.model_var_type == 'learned':\n",
    "            model_output, log_var = torch.split(model_output, 2, dim=-1)\n",
    "            var                   = torch.exp(log_var)\n",
    "\n",
    "        elif self.model_var_type in ['fixedsmall', 'fixedlarge']:\n",
    "\n",
    "            # below: only log_variance is used in the KL computations\n",
    "            var, log_var = {\n",
    "                # for 'fixedlarge', we set the initial (log-)variance like so to get a better decoder log likelihood\n",
    "                'fixedlarge': (self.betas, torch.log(torch.cat((self.posterior_variance[1].view(1, 1),\n",
    "                                                                self.betas[1:].view(-1, 1)), 0)).view(-1)),\n",
    "                'fixedsmall': (self.posterior_variance, self.posterior_log_variance_clipped),\n",
    "            }[self.model_var_type]\n",
    "\n",
    "            var     = extract(var, t, x.shape) * torch.ones_like(x)\n",
    "            log_var = extract(log_var, t, x.shape) * torch.ones_like(x)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_var_type)\n",
    "\n",
    "        # Mean parameterization\n",
    "        _maybe_clip = lambda x_: (x_.clamp(min=-1, max=1) if clip_denoised else x_)\n",
    "\n",
    "        if self.model_mean_type == 'xprev':\n",
    "            # the model predicts x_{t-1}\n",
    "            pred_x_0 = _maybe_clip(self.predict_start_from_prev(x_t=x, t=t, x_prev=model_output))\n",
    "            mean     = model_output\n",
    "        elif self.model_mean_type == 'xstart':\n",
    "            # the model predicts x_0\n",
    "            pred_x0    = _maybe_clip(model_output)\n",
    "            mean, _, _ = self.q_posterior_mean_variance(x_0=pred_x0, x_t=x, t=t)\n",
    "        elif self.model_mean_type == 'eps':\n",
    "            # the model predicts epsilon\n",
    "            pred_x0    = _maybe_clip(self.predict_start_from_noise(x_t=x, t=t, noise=model_output))\n",
    "            mean, _, _ = self.q_posterior_mean_variance(x_0=pred_x0, x_t=x, t=t)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_mean_type)\n",
    "\n",
    "        if return_pred_x0:\n",
    "            return mean, var, log_var, pred_x0\n",
    "        else:\n",
    "            return mean, var, log_var\n",
    "\n",
    "\n",
    "\n",
    "    def p_sample(self, model, x, t, noise_fn, clip_denoised=True, return_pred_x0=False, condition = torch.zeros(19)):\n",
    "        \"\"\"\n",
    "        t가 T...1일 때 mean, log_variance 이용해 x_(t-1) sampling, 논문의 Algorithm 2 참조\n",
    "        input:\n",
    "            noise_fn : 논문 상에서는 z로 표기\n",
    "        output:\n",
    "            sample : x_(t-1)\n",
    "        \"\"\"\n",
    "\n",
    "        mean, _, log_var, pred_x0 = self.p_mean_variance(model, x, t, clip_denoised, return_pred_x0=True, condition=condition)\n",
    "        noise                     = noise_fn(x.shape, dtype=x.dtype).to(x.device)\n",
    "\n",
    "        shape        = [x.shape[0]] + [1] * (x.ndim - 1)\n",
    "        nonzero_mask = (1 - (t == 0).type(torch.float32)).view(*shape).to(x.device)\n",
    "        sample       = mean + nonzero_mask * torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "        return (sample, pred_x0) if return_pred_x0 else sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, noise_fn=torch.randn, condition = torch.zeros(18)):\n",
    "        \"\"\"\n",
    "        t=T부터 0까지 이전 생성 image 이용해 sampling. t=0때 sampling 완료 후 image tensor return한다. 논문의 Algorithm 2 구현\n",
    "        input:\n",
    "            shape (tuple) : image tensor size \n",
    "        output:\n",
    "            img (tensor) : x_0에 해당하는 image tensor \n",
    "        \"\"\"\n",
    "\n",
    "        device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "        img    = noise_fn(shape).to(device)\n",
    "        img = torch.cat((img, condition),dim=1)\n",
    "        for i in reversed(range(self.num_timesteps)):\n",
    "            img = self.p_sample(\n",
    "                model,\n",
    "                img,\n",
    "                torch.full((shape[0],), i, dtype=torch.int64).to(device),\n",
    "                noise_fn=noise_fn,\n",
    "                return_pred_x0=False,\n",
    "                condition = condition\n",
    "            )\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop_progressive(self, model, shape, device, noise_fn=torch.randn, include_x0_pred_freq=50, ret_type=\"img\", condition= torch.zeros(19)):\n",
    "        \"\"\"\n",
    "        p_sample_loop와 유사. \n",
    "        input:\n",
    "            include_x0_pred_freq (int) : include_x0_pred_freq 번 마다 그 시점의 x_0 예측값을 기록\n",
    "            ret_type (string): img 면 image 하나를 return, list면 generation process의 모든 timestep의 image list를 return. \n",
    "        output:\n",
    "            x0_preds_ (tensor) : (B, num_recorded_x0_pred, C, H, W) size의 tensor\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        img = noise_fn(shape, dtype=torch.float32).to(device)\n",
    "\n",
    "        num_recorded_x0_pred = self.num_timesteps // include_x0_pred_freq\n",
    "        x0_preds_            = torch.zeros((shape[0], num_recorded_x0_pred, *shape[1:]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        img = img.view(shape[0],-1)\n",
    "        clip = lambda x_: (x_.clamp(min=-1, max=1))\n",
    "        imglist = [clip(img)]\n",
    "        predx0_list = []\n",
    "        for i in reversed(range(self.num_timesteps)):\n",
    "\n",
    "            # Sample p(x_{t-1} | x_t) as usual\n",
    "            img, pred_x0 = self.p_sample(model=model,\n",
    "                                         x=img,\n",
    "                                         t=torch.full((shape[0],), i, dtype=torch.int64).to(device),\n",
    "                                         noise_fn=noise_fn,\n",
    "                                         return_pred_x0=True,\n",
    "                                         condition = condition)\n",
    "            imglist.append(clip(img))\n",
    "            img = img.view(shape[0],-1)\n",
    "            \n",
    "            # Keep track of prediction of x0\n",
    "            insert_mask = np.floor(i // include_x0_pred_freq) == torch.arange(num_recorded_x0_pred,\n",
    "                                                                              dtype=torch.int32,\n",
    "                                                                              device=device)\n",
    "\n",
    "            insert_mask = insert_mask.to(torch.float32).view(1, num_recorded_x0_pred, *([1] * len(shape[1:])))\n",
    "            x0_preds_   = insert_mask * pred_x0[:, None, ...] + (1. - insert_mask) * x0_preds_\n",
    "            if i%include_x0_pred_freq == 0:\n",
    "                predx0_list.append(pred_x0)\n",
    "                print(i)\n",
    "        if ret_type == \"list\":\n",
    "            return imglist, x0_preds_, predx0_list\n",
    "        elif ret_type == \"img\":\n",
    "            return img, x0_preds_\n",
    "\n",
    "    # === Log likelihood calculation ===\n",
    "\n",
    "    def _vb_terms_bpd(self, model, x_0, x_t, t, clip_denoised, return_pred_x0, condition):\n",
    "        \"\"\"\n",
    "        논문에서 사용한 loss를 구하는 함수. (5)식 참조\n",
    "        t가 0인 image는 decoder의 negative log likelihood 값 return\n",
    "        t가 0이 아닌 image는 KL-Divergence return    \n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = t.shape[0]\n",
    "        true_mean, _, true_log_variance_clipped    = self.q_posterior_mean_variance(x_0=x_0,\n",
    "                                                                                    x_t=x_t,\n",
    "                                                                                    t=t)\n",
    "        model_mean, _, model_log_variance, pred_x0 = self.p_mean_variance(model,\n",
    "                                                                          x=x_t,\n",
    "                                                                          t=t,\n",
    "                                                                          clip_denoised=clip_denoised,\n",
    "                                                                          return_pred_x0=True,\n",
    "                                                                         condition = condition)\n",
    "\n",
    "        kl = normal_kl(true_mean, true_log_variance_clipped, model_mean, model_log_variance)\n",
    "        kl = torch.mean(kl.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(x_0, means=model_mean, log_scales=0.5 * model_log_variance)\n",
    "        decoder_nll = torch.mean(decoder_nll.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "\n",
    "        # At the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "        output = torch.where(t == 0, decoder_nll, kl)\n",
    "\n",
    "        return (output, pred_x0) if return_pred_x0 else output\n",
    "\n",
    "    def training_losses(self, model, x_0, cond, t, noise=None):\n",
    "        \"\"\"\n",
    "        loss type이 kl인 경우 위의 _vb_terms_bpd 값 return\n",
    "        mse인 경우 mse loss return    \n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "\n",
    "        x_t = self.q_sample(x_0=x_0, t=t, noise=noise)\n",
    "\n",
    "        \n",
    "        # Calculate the loss\n",
    "        if self.loss_type == 'kl':\n",
    "            # the variational bound\n",
    "            losses = self._vb_terms_bpd(model=model, x_0=x_0, x_t=x_t, t=t, clip_denoised=False, return_pred_x0=False)\n",
    "\n",
    "        elif self.loss_type == 'mse':\n",
    "            # unweighted MSE\n",
    "            assert self.model_var_type != 'learned'\n",
    "            target = {\n",
    "                'xprev': self.q_posterior_mean_variance(x_0=x_0, x_t=x_t, t=t)[0],\n",
    "                'xstart': x_0,\n",
    "                'eps': noise\n",
    "            }[self.model_mean_type]\n",
    "\n",
    "            model_output = model(x_t, cond, t)\n",
    "            losses = torch.mean((target - model_output.view(model_output.shape[0], -1)).view(x_0.shape[0], -1)**2, dim=1)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _prior_bpd(self, x_0):\n",
    "        \"\"\"\n",
    "        x_0의 prior distribution에 대한 복원 오차 계산하는 함수\n",
    "        \"\"\"\n",
    "\n",
    "        B, T                        = x_0.shape[0], self.num_timesteps\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_0,\n",
    "                                                           t=torch.full((B,), T - 1, dtype=torch.int64))\n",
    "        kl_prior                    = normal_kl(mean1=qt_mean,\n",
    "                                                logvar1=qt_log_variance,\n",
    "                                                mean2=torch.zeros_like(qt_mean),\n",
    "                                                logvar2=torch.zeros_like(qt_log_variance))\n",
    "\n",
    "        return torch.mean(kl_prior.view(B, -1), dim=1)/np.log(2.)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calc_bpd_loop(self, model, x_0, clip_denoised, condition):\n",
    "        \"\"\"\n",
    "        모든 timestep 에 대한 prior distribution에 대한 복원 오차 계산하는 함수    \n",
    "        \"\"\"\n",
    "\n",
    "        (B, C, H, W), T = x_0.shape, self.num_timesteps\n",
    "\n",
    "        new_vals_bt = torch.zeros((B, T))\n",
    "        new_mse_bt  = torch.zeros((B, T))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "\n",
    "            t_b = torch.full((B, ), t, dtype=torch.int64)\n",
    "\n",
    "            # Calculate VLB term at the current timestep\n",
    "            new_vals_b, pred_x0 = self._vb_terms_bpd(model=model,\n",
    "                                                     x_0=x_0,\n",
    "                                                     x_t=self.q_sample(x_0=x_0, t=t_b),\n",
    "                                                     t=t_b,\n",
    "                                                     clip_denoised=clip_denoised,\n",
    "                                                     return_pred_x0=True,\n",
    "                                                     condition = condition)\n",
    "\n",
    "            # MSE for progressive prediction loss\n",
    "            new_mse_b = torch.mean((pred_x0-x_0).view(B, -1)**2, dim=1)\n",
    "\n",
    "            # Insert the calculated term into the tensor of all terms\n",
    "            mask_bt = (t_b[:, None] == torch.arange(T)[None, :]).to(torch.float32)\n",
    "\n",
    "            new_vals_bt = new_vals_bt * (1. - mask_bt) + new_vals_b[:, None] * mask_bt\n",
    "            new_mse_bt  = new_mse_bt  * (1. - mask_bt) + new_mse_b[:, None] * mask_bt\n",
    "\n",
    "        prior_bpd_b = self._prior_bpd(x_0)\n",
    "        total_bpd_b = torch.sum(new_vals_bt, dim=1) + prior_bpd_b\n",
    "\n",
    "        return total_bpd_b, new_vals_bt, prior_bpd_b, new_mse_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee9f74-6c35-4f19-a47c-08772163f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adopted from https://github.com/rosinality/denoising-diffusion-pytorch with some minor changes.\n",
    "\n",
    "def swish(input):\n",
    "    \"\"\"\n",
    "    activation function. return x*sigmoid(x) \n",
    "    \"\"\"\n",
    "    return input * torch.sigmoid(input)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def variance_scaling_init_(tensor, scale=1, mode=\"fan_avg\", distribution=\"uniform\"):\n",
    "    \"\"\"\n",
    "    mode에 따라 scale이 바뀌고, 그 scale대로 weight tensor를 initialize 해준다.\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n",
    "\n",
    "    if mode == \"fan_in\":\n",
    "        scale /= fan_in\n",
    "\n",
    "    elif mode == \"fan_out\":\n",
    "        scale /= fan_out\n",
    "\n",
    "    else:\n",
    "        scale /= (fan_in + fan_out) / 2\n",
    "\n",
    "    if distribution == \"normal\":\n",
    "        std = math.sqrt(scale)\n",
    "\n",
    "        return tensor.normal_(0, std)\n",
    "\n",
    "    else:\n",
    "        bound = math.sqrt(3 * scale)\n",
    "\n",
    "        return tensor.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    in_channel,\n",
    "    out_channel,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    bias=True,\n",
    "    scale=1,\n",
    "    mode=\"fan_avg\",\n",
    "):\n",
    "    \"\"\"\n",
    "    convolution layer return하는 함수. \n",
    "    입력한 in_channel, out_channel, kernel_size, stride, padding 에 따라 convolution layer 만든다.\n",
    "    scale, mode에 따라 weight initialize 해주고 bias가 True면 0 bias를 가지도록 return.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv2d(\n",
    "        in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias\n",
    "    )\n",
    "\n",
    "    variance_scaling_init_(conv.weight, scale, mode=mode)\n",
    "\n",
    "    if bias:\n",
    "        nn.init.zeros_(conv.bias)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def linear(in_channel, out_channel, scale=1, mode=\"fan_avg\"):\n",
    "    \"\"\"\n",
    "    linear layer return 하는 함수.\n",
    "    in_channel, out_channel 에 따라 linear layer 만든다\n",
    "    scale, mode에 따라 weight initialize 해주고, 0 bias를 가지도록 해준다.\n",
    "    \"\"\"\n",
    "    lin = nn.Linear(in_channel, out_channel)\n",
    "\n",
    "    variance_scaling_init_(lin.weight, scale, mode=mode)\n",
    "    nn.init.zeros_(lin.bias)\n",
    "\n",
    "    return lin\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    위에서 정의한 swish 함수를 layer 형태로 만든 것\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return swish(input)\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Upsample과 conv2d를 layer로 가지는 nn.Sequential layer\n",
    "    channel을 scale factor만큼 키워준다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        layers = [\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            conv2d(channel, channel, 3, padding=1),\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Downsample(nn.Sequential):\n",
    "    \"\"\"\n",
    "    conv2d를 layer로 가지는 nn.Sequential layer. stride가 2라 output image의 크기가 줄어든다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        layers = [conv2d(channel, channel, 3, stride=2, padding=1)]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip connection을 사용한 network. time embedding 과 dropout을 hyperparameter로 가진다.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, time_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(32, in_channel)\n",
    "        self.activation1 = Swish()\n",
    "        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n",
    "\n",
    "        self.time = nn.Sequential(Swish(), linear(time_dim, out_channel))\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(32, out_channel)\n",
    "        self.activation2 = Swish()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n",
    "\n",
    "        if in_channel != out_channel:\n",
    "            self.skip = conv2d(in_channel, out_channel, 1)\n",
    "\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, input, time):\n",
    "        batch = input.shape[0]\n",
    "\n",
    "        out = self.conv1(self.activation1(self.norm1(input)))\n",
    "\n",
    "        out = out + self.time(time).view(batch, -1, 1, 1)\n",
    "\n",
    "        out = self.conv2(self.dropout(self.activation2(self.norm2(out))))\n",
    "\n",
    "        if self.skip is not None:\n",
    "            input = self.skip(input)\n",
    "\n",
    "        return out + input\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention 적용하는 layer\n",
    "    input의 channel 수 4배로 늘린 뒤 torch.chunk 로 q, k, v 나눔. \n",
    "    이때 channel의 수가 3의 배수 아닌데, query와 key의 channel수가 같고, value의 channel 수가 작음\n",
    "    나머지 계산의 dimension 계산은 아래 torch.einsum에 설명\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.GroupNorm(32, in_channel)\n",
    "        self.qkv = conv2d(in_channel, in_channel * 4, 1)\n",
    "        self.out = conv2d(in_channel, in_channel, 1, scale=1e-10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch, channel, height, width = input.shape\n",
    "\n",
    "        norm = self.norm(input)\n",
    "        qkv = self.qkv(norm)\n",
    "        query, key, value = qkv.chunk(3, dim=1)\n",
    "\n",
    "        attn = torch.einsum(\"nchw, ncyx -> nhwyx\", query, key).contiguous() / math.sqrt(\n",
    "            channel\n",
    "        )\n",
    "        attn = attn.view(batch, height, width, -1)\n",
    "        attn = torch.softmax(attn, -1)\n",
    "        attn = attn.view(batch, height, width, height, width)\n",
    "\n",
    "        out = torch.einsum(\"nhwyx, ncyx -> nchw\", attn, input).contiguous()\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out + input\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer의 sinusoidal positional embedding 적용\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim      = dim\n",
    "        half_dim      = self.dim // 2\n",
    "        self.inv_freq = torch.exp(torch.arange(half_dim, dtype=torch.float32) * (-math.log(10000) / (half_dim - 1)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        shape       = input.shape\n",
    "        input       = input.view(-1).to(torch.float32)\n",
    "        sinusoid_in = torch.ger(input, self.inv_freq.to(input.device))\n",
    "        pos_emb     = torch.cat([sinusoid_in.sin(), sinusoid_in.cos()], dim=-1)\n",
    "        pos_emb     = pos_emb.view(*shape, self.dim)\n",
    "        \n",
    "        return pos_emb\n",
    "\n",
    "\n",
    "class ResBlockWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet에서 기본단위로 사용되는 block\n",
    "    Resblock - SelfAttention으로 이루어짐\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, time_dim, dropout, use_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resblocks = ResBlock(in_channel, out_channel, time_dim, dropout)\n",
    "\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttention(out_channel)\n",
    "\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "    def forward(self, input, time):\n",
    "        out = self.resblocks(input, time)\n",
    "\n",
    "        if self.attention is not None:\n",
    "            out = self.attention(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def spatial_fold(input, fold):\n",
    "    \"\"\"\n",
    "    h, w 를 fold만큼 나눈 dim으로 바꿔주고, channel의 dimension을 그만큼 곱함\n",
    "    \"\"\"\n",
    "    if fold == 1:\n",
    "        return input\n",
    "\n",
    "    batch, channel, height, width = input.shape\n",
    "    h_fold = height // fold\n",
    "    w_fold = width // fold\n",
    "\n",
    "    return (\n",
    "        input.view(batch, channel, h_fold, fold, w_fold, fold)\n",
    "        .permute(0, 1, 3, 5, 2, 4)\n",
    "        .reshape(batch, -1, h_fold, w_fold)\n",
    "    )\n",
    "\n",
    "\n",
    "def spatial_unfold(input, unfold):\n",
    "    \"\"\"\n",
    "    spatial_fold와 반대.\n",
    "    h, w를 unfold만큼 곱한 dim으로 바꿔주고, channel의 dimension을 그만큼 나눔\n",
    "    \"\"\"\n",
    "    if unfold == 1:\n",
    "        return input\n",
    "\n",
    "    batch, channel, height, width = input.shape\n",
    "    h_unfold = height * unfold\n",
    "    w_unfold = width * unfold\n",
    "\n",
    "    return (\n",
    "        input.view(batch, -1, unfold, unfold, height, width)\n",
    "        .permute(0, 1, 4, 2, 5, 3)\n",
    "        .reshape(batch, -1, h_unfold, w_unfold)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    학습에 사용할 전체 모델.\n",
    "    dimension과 구조는 위의 markdown 그림 참조\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        channel,\n",
    "        channel_multiplier,\n",
    "        n_res_blocks,\n",
    "        attn_strides,\n",
    "        dropout=0,\n",
    "        fold=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fold = fold\n",
    "\n",
    "        time_dim = channel * 4\n",
    "        \n",
    "        self.enc_data = nn.Sequential(linear(numofcluster*100+18,3072))\n",
    "        \n",
    "        self.dec_data = nn.Sequential(linear(3072,numofcluster*100))\n",
    "\n",
    "        n_block = len(channel_multiplier)\n",
    "        \n",
    "\n",
    "        self.time = nn.Sequential(\n",
    "            TimeEmbedding(channel),\n",
    "            linear(channel, time_dim),\n",
    "            Swish(),\n",
    "            linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        down_layers   = [conv2d(in_channel * (fold ** 2), channel, 3, padding=1)]\n",
    "        feat_channels = [channel]\n",
    "        in_channel    = channel\n",
    "        for i in range(n_block):\n",
    "            for _ in range(n_res_blocks):\n",
    "                channel_mult = channel * channel_multiplier[i]\n",
    "\n",
    "                down_layers.append(\n",
    "                    ResBlockWithAttention(\n",
    "                        in_channel,\n",
    "                        channel_mult,\n",
    "                        time_dim,\n",
    "                        dropout,\n",
    "                        use_attention=2 ** i in attn_strides,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                feat_channels.append(channel_mult)\n",
    "                in_channel = channel_mult\n",
    "\n",
    "            if i != n_block - 1:\n",
    "                down_layers.append(Downsample(in_channel))\n",
    "                feat_channels.append(in_channel)\n",
    "\n",
    "        self.down = nn.ModuleList(down_layers)\n",
    "\n",
    "        self.mid = nn.ModuleList(\n",
    "            [\n",
    "                ResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    time_dim,\n",
    "                    dropout=dropout,\n",
    "                    use_attention=True,\n",
    "                ),\n",
    "                ResBlockWithAttention(\n",
    "                    in_channel, in_channel, time_dim, dropout=dropout\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        up_layers = []\n",
    "        for i in reversed(range(n_block)):\n",
    "            for _ in range(n_res_blocks + 1):\n",
    "                channel_mult = channel * channel_multiplier[i]\n",
    "\n",
    "                up_layers.append(\n",
    "                    ResBlockWithAttention(\n",
    "                        in_channel + feat_channels.pop(),\n",
    "                        channel_mult,\n",
    "                        time_dim,\n",
    "                        dropout=dropout,\n",
    "                        use_attention=2 ** i in attn_strides,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                in_channel = channel_mult\n",
    "\n",
    "            if i != 0:\n",
    "                up_layers.append(Upsample(in_channel))\n",
    "\n",
    "        self.up = nn.ModuleList(up_layers)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_channel),\n",
    "            Swish(),\n",
    "            conv2d(in_channel, 3 * (fold ** 2), 3, padding=1, scale=1e-10),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, condition,time):\n",
    "        \n",
    "        batch = input.shape[0]\n",
    "        x = torch.cat((input, condition),dim=1)\n",
    "        \n",
    "        x=self.enc_data(x)\n",
    "        time_embed = self.time(time)\n",
    "        feats = []\n",
    "        out = x.view(batch, 3,32,32)\n",
    "        \n",
    "        out = spatial_fold(out, self.fold)\n",
    "        \n",
    "        for layer in self.down:\n",
    "            if isinstance(layer, ResBlockWithAttention):\n",
    "                out = layer(out, time_embed)\n",
    "\n",
    "            else:\n",
    "                out = layer(out)\n",
    "\n",
    "            feats.append(out)\n",
    "\n",
    "        for layer in self.mid:\n",
    "            out = layer(out, time_embed)\n",
    "\n",
    "        for layer in self.up:\n",
    "            if isinstance(layer, ResBlockWithAttention):\n",
    "                out = layer(torch.cat((out, feats.pop()), 1), time_embed)\n",
    "\n",
    "            else:\n",
    "                out = layer(out)\n",
    "\n",
    "        out = self.out(out)\n",
    "        out = spatial_unfold(out, self.fold)\n",
    "        \n",
    "        out = out.view(batch, 3072)\n",
    "        kk = self.dec_data(out)\n",
    "        \n",
    "        return kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size//3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(input_size//3, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_size//3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(input_size//3, input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "df = new_data\n",
    "df_dict = {}\n",
    "df_encoded_dict = {}\n",
    "\n",
    "movieid = df.iloc[:-1,0]\n",
    "group_gmm_values = df.loc[df.index[-1], :]\n",
    "\n",
    "for group_value in group_gmm_values.unique():\n",
    "    group_df = df.loc[:, df.loc[df.index[-1], :] == group_value]\n",
    "    \n",
    "    df_dict[group_value] = group_df.iloc[:-1,:] # original data save\n",
    "    \n",
    "autoencoders = {}\n",
    "clusternum = {}\n",
    "for group_num in df_dict.keys():\n",
    "    if type(group_num)!=str:\n",
    "        print(group_num)\n",
    "        data = df_dict[group_num]\n",
    "        data = data.replace(np.nan, 2.5)\n",
    "        input_data = data.values\n",
    "        df_dict[group_num] = input_data\n",
    "        del(data)\n",
    "        gc.collect()\n",
    "        \n",
    "        # Convert the input data to a PyTorch tensor\n",
    "        print(input_data, input_data.shape)\n",
    "        clusternum[group_num] = input_data.shape[1]\n",
    "        input_tensor = torch.from_numpy(input_data).float().to(\"cuda\")\n",
    "\n",
    "        # Initialize the autoencoder model\n",
    "        input_size = input_data.shape[1] \n",
    "        encoding_dim = input_size  \n",
    "        autoencoder_new = Autoencoder(input_size, 100).to(\"cuda\")\n",
    "        if torch.cuda.device_count()>1:\n",
    "            autoencoder_new = nn.DataParallel(autoencoder_new)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(autoencoder_new.parameters(), lr=0.0001)\n",
    "        \n",
    "        # Training the autoencoder\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            _, outputs = autoencoder_new(input_tensor)\n",
    "            loss = criterion(outputs, input_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch%100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        autoencoders[group_num]=autoencoder_new\n",
    "\n",
    "        \n",
    "for group_num in df_dict.keys():\n",
    "    if type(group_num)!=str:\n",
    "        print(group_num)\n",
    "        encoded, decoded = autoencoders[group_num](torch.from_numpy(df_dict[group_num]).float().to(\"cuda\"))\n",
    "        df_encoded_dict[group_num] = encoded\n",
    "        decoded = decoded\n",
    "        print(decoded)\n",
    "        print(decoded.min(), decoded.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_condition = \"./data/data_delete_nogenres_1m.csv\"\n",
    "cond_data = pd.read_csv(data_condition)\n",
    "\n",
    "for num, j in enumerate(movieid):\n",
    "    temp_cond = torch.tensor(cond_data.loc[cond_data[\"movieId\"]==int(j)].iloc[:,2:].values,dtype = torch.float32)\n",
    "    if temp_cond.shape[0]==0:\n",
    "        print(j)\n",
    "    if num==0:\n",
    "        condition = temp_cond\n",
    "        \n",
    "    else:\n",
    "        condition = torch.cat((condition, temp_cond), dim=0)\n",
    "        \n",
    "for i in df_encoded_dict.keys():\n",
    "    try:\n",
    "        input_data = torch.cat((input_data, df_encoded_dict[i]),dim=1)\n",
    "\n",
    "\n",
    "    except:\n",
    "        input_data = df_encoded_dict[i]\n",
    "\n",
    "print(input_data.shape)\n",
    "print(condition.shape)\n",
    "\n",
    "middle_input = (input_data.max()+input_data.min())/2\n",
    "halfrange = (input_data.max()-input_data.min()) / 2\n",
    "\n",
    "input_data = (input_data - middle_input) / halfrange\n",
    "\n",
    "print(\"middle = \", middle_input, \"halfrange =\", halfrange)\n",
    "\n",
    "print(input_data.max(), input_data.min())\n",
    "Totaldata = (input_data, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed456a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0293b81-9319-40f5-969e-b90f47529e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_train_data(conf, inputdata):\n",
    "    if conf.dataset.name == 'movielens':\n",
    "        data, condition = inputdata\n",
    "        data = data.to(\"cpu\").detach()\n",
    "        condition = condition.detach()\n",
    "        datatuple = (data, condition)\n",
    "        dataset = TensorDataset(*datatuple)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_size = 3000\n",
    "        train_set = Subset(dataset, range(train_size))\n",
    "        valid_set = Subset(dataset, range(train_size, data.shape[0]))\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a0fc4-d8d5-47f2-92eb-248cd8a82b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## util 복붙\n",
    "\n",
    "class obj(object):\n",
    "    \"\"\"\n",
    "    config가 json 형식으로 되어있습니다.\n",
    "    json 파일의 데이터를 받아오는 형식을 바꿔주는 함수입니다. \n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "               setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.9999):\n",
    "    \"\"\"\n",
    "    아래 DDP class에서 EMA를 진행하기 위한 함수입니다.\n",
    "    model1의 parameter을 기존 param*decay + model2의 param*(1-decay)로 update 합니다.\n",
    "    \"\"\"\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def samples_fn(model, diffusion, shape, condition):\n",
    "    \"\"\"\n",
    "    sample image의 현재 범위는 -1~1입니다.\n",
    "    이를 image화 시키려먼 0~1 사이의 범위로 만들어주어야 하고, 이를 수행하는 함수입니다.\n",
    "    \"\"\"\n",
    "    samples = diffusion.p_sample_loop(model=model,\n",
    "                                      shape=shape,\n",
    "                                      noise_fn=torch.randn,\n",
    "                                     condition = condition)\n",
    "    return {\n",
    "      'samples': (samples + 1)/2\n",
    "    }\n",
    "\n",
    "\n",
    "def progressive_samples_fn(model, diffusion, shape, device, include_x0_pred_freq=50, ret_type = \"img\", condition= torch.zeros(19)):\n",
    "    \"\"\"\n",
    "    위 samples_fn과 역할이 동일합니다.\n",
    "    \"\"\"\n",
    "    samples, progressive_samples, pro_list = diffusion.p_sample_loop_progressive(\n",
    "        model=model,\n",
    "        shape=shape,\n",
    "        noise_fn=torch.randn,\n",
    "        device=device,\n",
    "        include_x0_pred_freq=include_x0_pred_freq,\n",
    "        ret_type = ret_type,\n",
    "        condition = condition\n",
    "    )\n",
    "    if ret_type == \"list\":\n",
    "        for i in range(len(samples)):\n",
    "            samples[i] = (samples[i] + 1)/2\n",
    "        for j in range(len(pro_list)):\n",
    "            pro_list[j] = (pro_list[j]+1)/2\n",
    "        return {'samples': samples, 'progressive_samples': (progressive_samples + 1)/2, \"pro_list\" : pro_list}\n",
    "    elif ret_type == \"img\":\n",
    "        samples = (samples +1)/2\n",
    "        return {'samples': samples, 'progressive_samples': (progressive_samples + 1)/2}\n",
    "\n",
    "\n",
    "def bpd_fn(model, diffusion, x, condition):\n",
    "    \"\"\"\n",
    "    복원 오차 계산하는 함수. diffusion.calc_bpd_loop 참고\n",
    "    \"\"\"\n",
    "    total_bpd_b, terms_bpd_bt, prior_bpd_b, mse_bt = diffusion.calc_bpd_loop(model=model, x_0=x, clip_denoised=True, condition = condition)\n",
    "\n",
    "    return {\n",
    "      'total_bpd': total_bpd_b,\n",
    "      'terms_bpd': terms_bpd_bt,\n",
    "      'prior_bpd': prior_bpd_b,\n",
    "      'mse': mse_bt\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(val_loader, model, diffusion, condition):\n",
    "    \"\"\"\n",
    "    validation dataset에 대해 bpd, mse 계산하는 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    bpd = []\n",
    "    mse = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(iter(val_loader)):\n",
    "            x       = x\n",
    "            metrics = bpd_fn(model, diffusion, x, condition)\n",
    "\n",
    "            bpd.append(metrics['total_bpd'].view(-1, 1))\n",
    "            mse.append(metrics['mse'].view(-1, 1))\n",
    "\n",
    "        bpd = torch.cat(bpd, dim=0).mean()\n",
    "        mse = torch.cat(mse, dim=0).mean()\n",
    "\n",
    "    return bpd, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556529d-326b-469a-b7d2-3515bd844058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DDP(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, conf, inputdata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conf  = conf\n",
    "        self.save_hyperparameters()\n",
    "        self.input = inputdata\n",
    "\n",
    "        self.model = UNet(self.conf.model.in_channel,\n",
    "                          self.conf.model.channel,\n",
    "                          channel_multiplier=self.conf.model.channel_multiplier,\n",
    "                          n_res_blocks=self.conf.model.n_res_blocks,\n",
    "                          attn_strides=self.conf.model.attn_strides,\n",
    "                          dropout=self.conf.model.dropout,\n",
    "                          fold=self.conf.model.fold\n",
    "                          )\n",
    "        self.ema   = UNet(self.conf.model.in_channel,\n",
    "                          self.conf.model.channel,\n",
    "                          channel_multiplier=self.conf.model.channel_multiplier,\n",
    "                          n_res_blocks=self.conf.model.n_res_blocks,\n",
    "                          attn_strides=self.conf.model.attn_strides,\n",
    "                          dropout=self.conf.model.dropout,\n",
    "                          fold=self.conf.model.fold\n",
    "                          )\n",
    "\n",
    "        self.betas = make_beta_schedule(schedule=self.conf.model.schedule.type,\n",
    "                                        start=self.conf.model.schedule.beta_start,\n",
    "                                        end=self.conf.model.schedule.beta_end,\n",
    "                                        n_timestep=self.conf.model.schedule.n_timestep)\n",
    "\n",
    "        self.diffusion = GaussianDiffusion(betas=self.betas,\n",
    "                                           model_mean_type=self.conf.model.mean_type,\n",
    "                                           model_var_type=self.conf.model.var_type,\n",
    "                                           loss_type=self.conf.model.loss_type)\n",
    "        \n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_set, self.valid_set = get_train_data(self.conf, self.input)\n",
    "        \n",
    "\n",
    "    def forward(self, x, condition):\n",
    "\n",
    "        return self.diffusion.p_sample_loop(self.model, x.shape, condition = condition)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        if self.conf.training.optimizer.type == 'adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.conf.training.optimizer.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "\n",
    "        img, cond = batch[0], batch[1]\n",
    "        time   = (torch.rand(img.shape[0]) * 1000).type(torch.int64).to(img.device)\n",
    "        loss   = self.diffusion.training_losses(self.model, img, cond, time).mean()\n",
    "\n",
    "        accumulate(self.ema, self.model.module if isinstance(self.model, nn.DataParallel) else self.model, 0.9999)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_loader = DataLoader(self.train_set,\n",
    "                                  batch_size=self.conf.training.dataloader.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=self.conf.training.dataloader.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=self.conf.training.dataloader.drop_last)\n",
    "        return train_loader\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        img, cond = batch[0], batch[1]\n",
    "        time   = (torch.rand(img.shape[0]) * 1000).type(torch.int64).to(img.device)\n",
    "        loss   = self.diffusion.training_losses(self.ema, img, cond, time).mean()\n",
    "\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss         = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "\n",
    "        \n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = DataLoader(self.valid_set,\n",
    "                                  batch_size=self.conf.validation.dataloader.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=self.conf.validation.dataloader.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=self.conf.validation.dataloader.drop_last)\n",
    "\n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebb304-481e-4825-aa23-84243cc73d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"train\": False,\n",
    "        \"config\": 'config/diffusion_movielens.json',\n",
    "        \"ckpt_dir\": './ckpts',\n",
    "        \"ckpt_freq\": 5,\n",
    "        \"n_gpu\": 1,\n",
    "        \"model_dir\": './ckpts/last.ckpt',\n",
    "        \"sample_dir\": 'samples',\n",
    "        \"prog_sample_freq\": 100,\n",
    "        \"n_samples\": 100\n",
    "    })\n",
    "\n",
    "    \n",
    "path_to_config = args.config\n",
    "with open(path_to_config, 'r') as f:\n",
    "    conf = json.load(f)\n",
    "\n",
    "conf = obj(conf)\n",
    "denoising_diffusion_model = DDP(conf, Totaldata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c476e-b937-4388-a662-939a85dc7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pl.__version__ == '0.8.5':\n",
    "    checkpoint_callback = ModelCheckpoint(filepath=os.path.join(args.ckpt_dir, 'ddp_{epoch:02d}-{val_loss:.2f}'),\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=True,\n",
    "                                      save_last=True,\n",
    "                                      save_top_k=-1,\n",
    "                                      save_weights_only=True,\n",
    "                                      mode='auto',\n",
    "                                      period=args.ckpt_freq,\n",
    "                                      prefix='')\n",
    "    \n",
    "\n",
    "#     except:\n",
    "#         print('The present library version of pytorch lightning is ',pl.__version__)\n",
    "#         print('Please check if library requirements are satisfied')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f0241-57e1-4455-a8e7-44030dc39e1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = pl.Trainer(fast_dev_run=False,\n",
    "                     gpus=args.n_gpu,\n",
    "                     max_steps=conf.training.n_iter,\n",
    "                     precision=conf.model.precision,\n",
    "                     gradient_clip_val=1.,\n",
    "                     progress_bar_refresh_rate=1,\n",
    "                     checkpoint_callback=checkpoint_callback,\n",
    "                     check_val_every_n_epoch=50,\n",
    "                     max_epochs = 100,\n",
    "                     distributed_backend='dp')\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(denoising_diffusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5047af0-98e1-4d6b-b6a5-91f972e52f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(denoising_diffusion_model.state_dict(), f\"./trained_ckpt/model_1m_{clustertype}={numofcluster}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe07af4-05ea-4218-a87d-e0f4cb4e659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, key in enumerate(autoencoders.keys()):\n",
    "    torch.save(autoencoders[key].state_dict(),f\"./trained_ckpt/model_1m_{clustertype}={numofcluster}_AE{key}.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
