{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36bd49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.9.0+cu111 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 2)) (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu111 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 3)) (0.10.0+cu111)\n",
      "Requirement already satisfied: torchaudio===0.9.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: torchdiffeq in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: tensorflow-gpu==2.4.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.12.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: silence_tensorflow in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: pytorch-lightning==0.8.5 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 9)) (0.8.5)\n",
      "Requirement already satisfied: numpy==1.19.2 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 10)) (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 11)) (3.6.3)\n",
      "Requirement already satisfied: Pillow==9.5.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 12)) (9.5.0)\n",
      "Requirement already satisfied: easydict in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 13)) (1.10)\n",
      "Requirement already satisfied: setuptools==59.5.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 14)) (59.5.0)\n",
      "Requirement already satisfied: pandas==1.3.5 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from -r ./requirements_for_cuda111.txt (line 15)) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from torch==1.9.0+cu111->-r ./requirements_for_cuda111.txt (line 2)) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (3.19.6)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.38.4)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.32.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-probability==0.12.1->-r ./requirements_for_cuda111.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-probability==0.12.1->-r ./requirements_for_cuda111.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorflow-probability==0.12.1->-r ./requirements_for_cuda111.txt (line 7)) (0.1.8)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from pytorch-lightning==0.8.5->-r ./requirements_for_cuda111.txt (line 9)) (0.18.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from pytorch-lightning==0.8.5->-r ./requirements_for_cuda111.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from pytorch-lightning==0.8.5->-r ./requirements_for_cuda111.txt (line 9)) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from pandas==1.3.5->-r ./requirements_for_cuda111.txt (line 15)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from pandas==1.3.5->-r ./requirements_for_cuda111.txt (line 15)) (2022.7)\n",
      "Requirement already satisfied: scipy>=1.4.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from torchdiffeq->-r ./requirements_for_cuda111.txt (line 5)) (1.9.3)\n",
      "Requirement already satisfied: support-developer in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from silence_tensorflow->-r ./requirements_for_cuda111.txt (line 8)) (1.0.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from matplotlib->-r ./requirements_for_cuda111.txt (line 11)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from matplotlib->-r ./requirements_for_cuda111.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from matplotlib->-r ./requirements_for_cuda111.txt (line 11)) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from matplotlib->-r ./requirements_for_cuda111.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from matplotlib->-r ./requirements_for_cuda111.txt (line 11)) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from matplotlib->-r ./requirements_for_cuda111.txt (line 11)) (3.0.9)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.16.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from tqdm>=4.41.0->pytorch-lightning==0.8.5->-r ./requirements_for_cuda111.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\envs\\diffusion\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0->-r ./requirements_for_cuda111.txt (line 6)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -r ./requirements_for_cuda111.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a251321-3315-4044-85f3-5e7d89d089b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\diffusion\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets\n",
    "from torch.utils.data import Subset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import easydict\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "############################### Change this term for your input dataset ###############################\n",
    "\n",
    "numofcluster = 5\n",
    "clustertype = \"1st\"\n",
    "\n",
    "path = \"./data/\"\n",
    "filename1 = path + f\"ml_1m_user_mov_{clustertype}_cluster={numofcluster}.csv\"\n",
    "new_data = pd.read_csv(filename1)\n",
    "\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dec027f-ce1a-46f6-8d1d-26ac98c05d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n",
      "0.8.5\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # 1.9.0+cu1x1 ??\n",
    "print(pl.__version__) # 0.8.5 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0f80e7-6ca9-4240-b942-34da06fc9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(schedule, start, end, n_timestep):\n",
    "    \"\"\"\n",
    "    입력한 scheduling 방식에 따라 forward process에서 사용되는 beta를 만들어주는 함수\n",
    "    input : \n",
    "        schedule (str) : scheduling 방식\n",
    "        start, end (float) : beta 1, beta T 에 해당하는 값\n",
    "        n_timestep (int) : T \n",
    "    output : \n",
    "        betas (tensor (n_stepsize))\n",
    "    \"\"\" \n",
    "    if schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "    elif schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, n_timestep, dtype=torch.float64)\n",
    "    elif schedule == 'warmup10':\n",
    "        betas = _warmup_beta(start, end, n_timestep, 0.1)\n",
    "    elif schedule == 'warmup50':\n",
    "        betas = _warmup_beta(start, end, n_timestep, 0.5)\n",
    "    elif schedule == 'const':\n",
    "        betas = end * torch.ones(n_timestep, dtype=torch.float64)\n",
    "    elif schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        betas = 1. / (torch.linspace(n_timestep, 1, n_timestep, dtype=torch.float64))\n",
    "    else:\n",
    "        raise NotImplementedError(schedule)\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def _warmup_beta(start, end, n_timestep, warmup_frac):\n",
    "    \"\"\"\n",
    "    make_beta_schedule에서 schedule이 warmup10, warmup50 일 때 사용되는 함수\n",
    "    warmup_time만큼 증가, 그 이후로는 end 값으로 고정\n",
    "    input : \n",
    "        start, end (float) : beta 1, beta T 에 해당하는 값\n",
    "        n_timestep (int) : T\n",
    "        warmup_frac (float) : warmup_time의 비율\n",
    "    output : \n",
    "        betas (tensor (n_stepsize))\n",
    "    \"\"\"\n",
    "    betas               = end * torch.ones(n_timestep, dtype=torch.float64)\n",
    "    warmup_time         = int(n_timestep * warmup_frac)\n",
    "    betas[:warmup_time] = torch.linspace(start, end, warmup_time, dtype=torch.float64)\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    두 normal distribution의 mean, log-variance가 주어졌을 때 kl divergence를 계산하는 함수\n",
    "    input:\n",
    "        mean1, logvar1, mean2, logvar2 (float)\n",
    "    output:\n",
    "        kl-divergence (float)\n",
    "    \"\"\"\n",
    "\n",
    "    kl = 0.5 * (-1.0 + logvar2 - logvar1 + torch.exp(logvar1 - logvar2) + ((mean1 - mean2) ** 2) * torch.exp(-logvar2))\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "def extract(input, t, shape):\n",
    "    \"\"\"\n",
    "    input tensor에서 첫번째 차원을 기준으로 index가 t인 값 추출, output의 shape은 shape[0], 1,...,1\n",
    "    timestep에 맞는 값을 추출을 위해 사용. 예를 들어 input이 betas면, extract한 값은 beta_{t}\n",
    "    input:\n",
    "        input (tensor)\n",
    "        t (tensor) : batch 길이 tensor, 내부는 모두 time값으로 채워져있음. tensor([1,1,...1])\n",
    "        shape (tuple)\n",
    "    output:\n",
    "        out (tensor)\n",
    "    \"\"\"\n",
    "    out     = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [shape[0]] + [1] * (len(shape) - 1)\n",
    "    out     = out.reshape(*reshape)\n",
    "    return out\n",
    "\n",
    "\n",
    "def noise_like(shape, noise_fn, repeat=False):\n",
    "    \"\"\"\n",
    "    입력한 repeat function에 따라 noise를 만들어주는 함수. \n",
    "    repeat이 false면 shape[0]을 첫번째 dim의 size로 하고, 나머지 dim의 size는 1인 noise tensor 생성. \n",
    "    repeat이 True면 shape의 dim을 가진 noise tensor 생성\n",
    "    input:\n",
    "        shape (tensor)\n",
    "        noise_fn (function)\n",
    "    output:\n",
    "        shape에 맞는 noise tensor (tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    if repeat:\n",
    "        resid = [1] * (len(shape) - 1)\n",
    "        shape_one = (1, *shape[1:])\n",
    "\n",
    "        return noise_fn(*shape_one).repeat(shape[0], *resid)\n",
    "\n",
    "    else:\n",
    "        return noise_fn(*shape)\n",
    "\n",
    "\n",
    "def approx_standard_normal_cdf(x):\n",
    "    \"\"\"\n",
    "    standard normal 의 CDF(x)의 approximation\n",
    "    input:\n",
    "        x (float)\n",
    "    output:\n",
    "        approx. of integral -infty to x of standard normal (float)\n",
    "    \"\"\"\n",
    "    return 0.5 * (1.0 + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def discretized_gaussian_log_likelihood(x, *, means, log_scales):\n",
    "    \"\"\"\n",
    "    input x는 [0,255]의 int로 scaling 되어있고, 이를 [-1,1]로 rescale한 후 discretized된 log likelihood 계산하는 함수\n",
    "    논문 3.3 Data scaling~ 부분 참조\n",
    "    input:\n",
    "        x (tensor) : x_0, generated image\n",
    "        means (tensor) : mean of model at t=1\n",
    "        log_scales (tensor) : 0.5*log_variance of model at t=1  \n",
    "    output:\n",
    "        log_probs (tensor) : 각 xi 의 log_probability를 원소로 하는 tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Assumes data is integers [0, 255] rescaled to [-1, 1]\n",
    "    centered_x = x - means\n",
    "    inv_stdv   = torch.exp(-log_scales)\n",
    "    plus_in    = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus   = approx_standard_normal_cdf(plus_in)\n",
    "    min_in     = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min    = approx_standard_normal_cdf(min_in)\n",
    "\n",
    "    log_cdf_plus          = torch.log(torch.clamp(cdf_plus, min=1e-12))\n",
    "    log_one_minus_cdf_min = torch.log(torch.clamp(1 - cdf_min, min=1e-12))\n",
    "    cdf_delta             = cdf_plus - cdf_min\n",
    "    log_probs             = torch.where(x < -0.999, log_cdf_plus,\n",
    "                                        torch.where(x > 0.999, log_one_minus_cdf_min,\n",
    "                                                    torch.log(torch.clamp(cdf_delta, min=1e-12))))\n",
    "\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2df791-80df-45da-b2d7-cfd7bde6d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, betas, model_mean_type, model_var_type, loss_type):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            betas (tensor) : 미리 정해진 forward process 의 variance schedule, 1- (size = (n_timesteps) )\n",
    "            model_mean_type (str) : 논문 상에서는 eps 사용\n",
    "            model_var_type (str) : fixedsmall, fixedlarge가 각각 논문 3.2장에서 나온 두 종류의 variance. \n",
    "            loss_type (str) : 논문 상에서는 kl 사용\n",
    "        class 변수:\n",
    "            self.register로 만들어진 변수들 : betas를 통해 계산할 수 있는 값들. 추후 likelihood 계산에 바로 사용하기 위해 미리 계산\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        betas              = betas.type(torch.float64)\n",
    "        timesteps          = betas.shape[0]\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        self.model_mean_type = model_mean_type  # xprev, xstart, eps\n",
    "        self.model_var_type  = model_var_type   # learned, fixedsmall, fixedlarge\n",
    "        self.loss_type       = loss_type        # kl, mse\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, 0)\n",
    "        alphas_cumprod_prev = torch.cat(\n",
    "            (torch.tensor([1], dtype=torch.float64), alphas_cumprod[:-1]), 0\n",
    "        )\n",
    "        posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "\n",
    "        self.register(\"betas\", betas)\n",
    "        self.register(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n",
    "\n",
    "        self.register(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1 - alphas_cumprod))\n",
    "        self.register(\"log_one_minus_alphas_cumprod\", torch.log(1 - alphas_cumprod))\n",
    "        self.register(\"sqrt_recip_alphas_cumprod\", torch.rsqrt(alphas_cumprod))\n",
    "        self.register(\"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1 / alphas_cumprod - 1))\n",
    "        self.register(\"posterior_variance\", posterior_variance)\n",
    "        self.register(\"posterior_log_variance_clipped\",\n",
    "                      torch.log(torch.cat((posterior_variance[1].view(1, 1),\n",
    "                                           posterior_variance[1:].view(-1, 1)), 0)).view(-1))\n",
    "        self.register(\"posterior_mean_coef1\", (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)))\n",
    "        self.register(\"posterior_mean_coef2\", ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)))\n",
    "\n",
    "    def register(self, name, tensor):\n",
    "        \"\"\"\n",
    "        class 변수 등록을 위한 함수, class 선언 시에 변수로 설정된다.\n",
    "        input:\n",
    "            name (str) : 등록할 이름\n",
    "            tensor (tensor) : 등록할 tensor\n",
    "        \"\"\"\n",
    "        self.register_buffer(name, tensor.type(torch.float32))\n",
    "\n",
    "        \n",
    "    # === forward process ===\n",
    "    \n",
    "    \n",
    "    def q_mean_variance(self, x_0, t):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0)의 mean, variance, log_variance를 return하는 함수, 논문 (4)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch (cifar10 기준 size (32,3,32,32))\n",
    "        output:\n",
    "            mean, variance, log_variance : t 시점의 q의 mean, variance, log-variance (size는 x_0와 같음)\n",
    "        \"\"\"\n",
    "        mean = extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "        variance = extract(1. - self.alphas_cumprod, t, x_0.shape)\n",
    "        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0) sampling 하는 함수, 논문 (4)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch \n",
    "        output:\n",
    "            t 시점의 q(x_t|x_0)을 sampling 한 tensor \n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        return (extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0\n",
    "                + extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape) * noise)\n",
    "    \n",
    "    def q_sample_loop(self, x_0, T, device, freq=50):\n",
    "        \"\"\"\n",
    "        q(x_t|x_0)들의 list를 return\n",
    "        t는 0~T 까지 return할 수 있으며 freq에 따라 return list의 size가 변경된다.\n",
    "        input:\n",
    "            freq (int): freq step마다 imglist에 append해준다. 예를 들어 freq가 50이면 x_0, x_50, x_100...을 imglist에 넣어준다\n",
    "        output:\n",
    "            imglist (list of tensor)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        imglist = [x_0]\n",
    "\n",
    "        clip = lambda x_: (x_.clamp(min=-1, max=1) )\n",
    "        for i in range(T+1):\n",
    "            if (i+1)%freq == 0:\n",
    "                imglist.append(clip(self.q_sample(x_0.to(device), torch.full((128,) , i).to(device) , noise.to(device))))\n",
    "        return imglist\n",
    "    \n",
    "    def q_posterior_mean_variance(self, x_0, x_t, t):\n",
    "        \"\"\"\n",
    "        q(x_(t-1)|x_t,x_0) 의 mean, variance, log_variance return하는 함수, 논문 (6), (7)식 참조\n",
    "        input:\n",
    "            x_0 (tensor): input image batch \n",
    "            x_t (tensor): forward process를 timestep t만큼 했을 때\n",
    "        output:\n",
    "            mean, var, log_var_clipped : mean, variance, log-variance of q(x_(t-1)|x_t,x_0) \n",
    "        \"\"\"\n",
    "        mean            = (extract(self.posterior_mean_coef1, t, x_t.shape) * x_0\n",
    "                           + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t)\n",
    "        var             = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "\n",
    "        return mean, var, log_var_clipped\n",
    "\n",
    "    \n",
    "    # === reverse process ===\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"\n",
    "        mean type이 eps인 경우 x_0 예측, (12)식 변형\n",
    "        \"\"\"\n",
    "        \n",
    "        x_t = x_t.view(noise.shape)\n",
    "        \n",
    "        \n",
    "        return (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "                - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise)\n",
    "\n",
    "    def predict_start_from_prev(self, x_t, t, x_prev):\n",
    "        \"\"\"\n",
    "        mean type이 xprev인 경우 x_0 예측, (7)식 변형    \n",
    "        \"\"\"\n",
    "\n",
    "        return (extract(1./self.posterior_mean_coef1, t, x_t.shape) * x_prev -\n",
    "                extract(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape) * x_t)\n",
    "    \n",
    "    def p_mean_variance(self, model, x, t, clip_denoised, return_pred_x0, condition):\n",
    "        \"\"\"\n",
    "        reverse step model의 mean, var type에 따라 다른 mean, variance, log_variance를 return하는 함수.\n",
    "        var type이 fixedsmall, fixedlarge가 각각 논문 3.2장에서 나온 두 종류의 variance. \n",
    "        mean type이 eps인 경우가 논문의 (12)식 변형, xprev인 경우가 논문의 (7)식 변경\n",
    "        input:\n",
    "            model (class) : 본 논문에서는 Unet model\n",
    "            cleap_denoised (bool) : clipping 여부 결정 \n",
    "            return_pred_x0 (bool) : 예측한 x0 return 여부 결정 \n",
    "            \n",
    "        output:\n",
    "            mean, var, log_var, pred_x0 : mean, variance, log_variance, predicted x0 of reverse process\n",
    "        \"\"\"\n",
    "\n",
    "        model_output = model(x, condition, t)\n",
    "\n",
    "        # Learned or fixed variance?\n",
    "        if self.model_var_type == 'learned':\n",
    "            model_output, log_var = torch.split(model_output, 2, dim=-1)\n",
    "            var                   = torch.exp(log_var)\n",
    "\n",
    "        elif self.model_var_type in ['fixedsmall', 'fixedlarge']:\n",
    "\n",
    "            # below: only log_variance is used in the KL computations\n",
    "            var, log_var = {\n",
    "                # for 'fixedlarge', we set the initial (log-)variance like so to get a better decoder log likelihood\n",
    "                'fixedlarge': (self.betas, torch.log(torch.cat((self.posterior_variance[1].view(1, 1),\n",
    "                                                                self.betas[1:].view(-1, 1)), 0)).view(-1)),\n",
    "                'fixedsmall': (self.posterior_variance, self.posterior_log_variance_clipped),\n",
    "            }[self.model_var_type]\n",
    "\n",
    "            var     = extract(var, t, x.shape) * torch.ones_like(x)\n",
    "            log_var = extract(log_var, t, x.shape) * torch.ones_like(x)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_var_type)\n",
    "\n",
    "        # Mean parameterization\n",
    "        _maybe_clip = lambda x_: (x_.clamp(min=-1, max=1) if clip_denoised else x_)\n",
    "\n",
    "        if self.model_mean_type == 'xprev':\n",
    "            # the model predicts x_{t-1}\n",
    "            pred_x_0 = _maybe_clip(self.predict_start_from_prev(x_t=x, t=t, x_prev=model_output))\n",
    "            mean     = model_output\n",
    "        elif self.model_mean_type == 'xstart':\n",
    "            # the model predicts x_0\n",
    "            pred_x0    = _maybe_clip(model_output)\n",
    "            mean, _, _ = self.q_posterior_mean_variance(x_0=pred_x0, x_t=x, t=t)\n",
    "        elif self.model_mean_type == 'eps':\n",
    "            # the model predicts epsilon\n",
    "            pred_x0    = _maybe_clip(self.predict_start_from_noise(x_t=x, t=t, noise=model_output))\n",
    "            mean, _, _ = self.q_posterior_mean_variance(x_0=pred_x0, x_t=x, t=t)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_mean_type)\n",
    "\n",
    "        if return_pred_x0:\n",
    "            return mean, var, log_var, pred_x0\n",
    "        else:\n",
    "            return mean, var, log_var\n",
    "\n",
    "\n",
    "\n",
    "    def p_sample(self, model, x, t, noise_fn, clip_denoised=True, return_pred_x0=False, condition = torch.zeros(19)):\n",
    "        \"\"\"\n",
    "        t가 T...1일 때 mean, log_variance 이용해 x_(t-1) sampling, 논문의 Algorithm 2 참조\n",
    "        input:\n",
    "            noise_fn : 논문 상에서는 z로 표기\n",
    "        output:\n",
    "            sample : x_(t-1)\n",
    "        \"\"\"\n",
    "\n",
    "        mean, _, log_var, pred_x0 = self.p_mean_variance(model, x, t, clip_denoised, return_pred_x0=True, condition=condition)\n",
    "        noise                     = noise_fn(x.shape, dtype=x.dtype).to(x.device)\n",
    "\n",
    "        shape        = [x.shape[0]] + [1] * (x.ndim - 1)\n",
    "        nonzero_mask = (1 - (t == 0).type(torch.float32)).view(*shape).to(x.device)\n",
    "        sample       = mean + nonzero_mask * torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "        return (sample, pred_x0) if return_pred_x0 else sample\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape, noise_fn=torch.randn, condition = torch.zeros(18)):\n",
    "        \"\"\"\n",
    "        t=T부터 0까지 이전 생성 image 이용해 sampling. t=0때 sampling 완료 후 image tensor return한다. 논문의 Algorithm 2 구현\n",
    "        input:\n",
    "            shape (tuple) : image tensor size \n",
    "        output:\n",
    "            img (tensor) : x_0에 해당하는 image tensor \n",
    "        \"\"\"\n",
    "\n",
    "        device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n",
    "        img    = noise_fn(shape).to(device)\n",
    "        img = torch.cat((img, condition),dim=1)\n",
    "        for i in reversed(range(self.num_timesteps)):\n",
    "            img = self.p_sample(\n",
    "                model,\n",
    "                img,\n",
    "                torch.full((shape[0],), i, dtype=torch.int64).to(device),\n",
    "                noise_fn=noise_fn,\n",
    "                return_pred_x0=False,\n",
    "                condition = condition\n",
    "            )\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop_progressive(self, model, shape, device, noise_fn=torch.randn, include_x0_pred_freq=50, ret_type=\"img\", condition= torch.zeros(19)):\n",
    "        \"\"\"\n",
    "        p_sample_loop와 유사. \n",
    "        input:\n",
    "            include_x0_pred_freq (int) : include_x0_pred_freq 번 마다 그 시점의 x_0 예측값을 기록\n",
    "            ret_type (string): img 면 image 하나를 return, list면 generation process의 모든 timestep의 image list를 return. \n",
    "        output:\n",
    "            x0_preds_ (tensor) : (B, num_recorded_x0_pred, C, H, W) size의 tensor\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        img = noise_fn(shape, dtype=torch.float32).to(device)\n",
    "\n",
    "        num_recorded_x0_pred = self.num_timesteps // include_x0_pred_freq\n",
    "        x0_preds_            = torch.zeros((shape[0], num_recorded_x0_pred, *shape[1:]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        img = img.view(shape[0],-1)\n",
    "        clip = lambda x_: (x_.clamp(min=-1, max=1))\n",
    "        imglist = [clip(img)]\n",
    "        predx0_list = []\n",
    "        for i in reversed(range(self.num_timesteps)):\n",
    "\n",
    "            # Sample p(x_{t-1} | x_t) as usual\n",
    "            img, pred_x0 = self.p_sample(model=model,\n",
    "                                         x=img,\n",
    "                                         t=torch.full((shape[0],), i, dtype=torch.int64).to(device),\n",
    "                                         noise_fn=noise_fn,\n",
    "                                         return_pred_x0=True,\n",
    "                                         condition = condition)\n",
    "            imglist.append(clip(img))\n",
    "            img = img.view(shape[0],-1)\n",
    "            \n",
    "            # Keep track of prediction of x0\n",
    "            insert_mask = np.floor(i // include_x0_pred_freq) == torch.arange(num_recorded_x0_pred,\n",
    "                                                                              dtype=torch.int32,\n",
    "                                                                              device=device)\n",
    "\n",
    "            insert_mask = insert_mask.to(torch.float32).view(1, num_recorded_x0_pred, *([1] * len(shape[1:])))\n",
    "            x0_preds_   = insert_mask * pred_x0[:, None, ...] + (1. - insert_mask) * x0_preds_\n",
    "            if i%include_x0_pred_freq == 0:\n",
    "                predx0_list.append(pred_x0)\n",
    "                print(i)\n",
    "        if ret_type == \"list\":\n",
    "            return imglist, x0_preds_, predx0_list\n",
    "        elif ret_type == \"img\":\n",
    "            return img, x0_preds_\n",
    "\n",
    "    # === Log likelihood calculation ===\n",
    "\n",
    "    def _vb_terms_bpd(self, model, x_0, x_t, t, clip_denoised, return_pred_x0, condition):\n",
    "        \"\"\"\n",
    "        논문에서 사용한 loss를 구하는 함수. (5)식 참조\n",
    "        t가 0인 image는 decoder의 negative log likelihood 값 return\n",
    "        t가 0이 아닌 image는 KL-Divergence return    \n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = t.shape[0]\n",
    "        true_mean, _, true_log_variance_clipped    = self.q_posterior_mean_variance(x_0=x_0,\n",
    "                                                                                    x_t=x_t,\n",
    "                                                                                    t=t)\n",
    "        model_mean, _, model_log_variance, pred_x0 = self.p_mean_variance(model,\n",
    "                                                                          x=x_t,\n",
    "                                                                          t=t,\n",
    "                                                                          clip_denoised=clip_denoised,\n",
    "                                                                          return_pred_x0=True,\n",
    "                                                                         condition = condition)\n",
    "\n",
    "        kl = normal_kl(true_mean, true_log_variance_clipped, model_mean, model_log_variance)\n",
    "        kl = torch.mean(kl.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(x_0, means=model_mean, log_scales=0.5 * model_log_variance)\n",
    "        decoder_nll = torch.mean(decoder_nll.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "\n",
    "        # At the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "        output = torch.where(t == 0, decoder_nll, kl)\n",
    "\n",
    "        return (output, pred_x0) if return_pred_x0 else output\n",
    "\n",
    "    def training_losses(self, model, x_0, cond, t, noise=None):\n",
    "        \"\"\"\n",
    "        loss type이 kl인 경우 위의 _vb_terms_bpd 값 return\n",
    "        mse인 경우 mse loss return    \n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "\n",
    "        x_t = self.q_sample(x_0=x_0, t=t, noise=noise)\n",
    "\n",
    "        \n",
    "        # Calculate the loss\n",
    "        if self.loss_type == 'kl':\n",
    "            # the variational bound\n",
    "            losses = self._vb_terms_bpd(model=model, x_0=x_0, x_t=x_t, t=t, clip_denoised=False, return_pred_x0=False)\n",
    "\n",
    "        elif self.loss_type == 'mse':\n",
    "            # unweighted MSE\n",
    "            assert self.model_var_type != 'learned'\n",
    "            target = {\n",
    "                'xprev': self.q_posterior_mean_variance(x_0=x_0, x_t=x_t, t=t)[0],\n",
    "                'xstart': x_0,\n",
    "                'eps': noise\n",
    "            }[self.model_mean_type]\n",
    "\n",
    "            model_output = model(x_t, cond, t)\n",
    "            losses = torch.mean((target - model_output.view(model_output.shape[0], -1)).view(x_0.shape[0], -1)**2, dim=1)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _prior_bpd(self, x_0):\n",
    "        \"\"\"\n",
    "        x_0의 prior distribution에 대한 복원 오차 계산하는 함수\n",
    "        \"\"\"\n",
    "\n",
    "        B, T                        = x_0.shape[0], self.num_timesteps\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_0,\n",
    "                                                           t=torch.full((B,), T - 1, dtype=torch.int64))\n",
    "        kl_prior                    = normal_kl(mean1=qt_mean,\n",
    "                                                logvar1=qt_log_variance,\n",
    "                                                mean2=torch.zeros_like(qt_mean),\n",
    "                                                logvar2=torch.zeros_like(qt_log_variance))\n",
    "\n",
    "        return torch.mean(kl_prior.view(B, -1), dim=1)/np.log(2.)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calc_bpd_loop(self, model, x_0, clip_denoised, condition):\n",
    "        \"\"\"\n",
    "        모든 timestep 에 대한 prior distribution에 대한 복원 오차 계산하는 함수    \n",
    "        \"\"\"\n",
    "\n",
    "        (B, C, H, W), T = x_0.shape, self.num_timesteps\n",
    "\n",
    "        new_vals_bt = torch.zeros((B, T))\n",
    "        new_mse_bt  = torch.zeros((B, T))\n",
    "\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "\n",
    "            t_b = torch.full((B, ), t, dtype=torch.int64)\n",
    "\n",
    "            # Calculate VLB term at the current timestep\n",
    "            new_vals_b, pred_x0 = self._vb_terms_bpd(model=model,\n",
    "                                                     x_0=x_0,\n",
    "                                                     x_t=self.q_sample(x_0=x_0, t=t_b),\n",
    "                                                     t=t_b,\n",
    "                                                     clip_denoised=clip_denoised,\n",
    "                                                     return_pred_x0=True,\n",
    "                                                     condition = condition)\n",
    "\n",
    "            # MSE for progressive prediction loss\n",
    "            new_mse_b = torch.mean((pred_x0-x_0).view(B, -1)**2, dim=1)\n",
    "\n",
    "            # Insert the calculated term into the tensor of all terms\n",
    "            mask_bt = (t_b[:, None] == torch.arange(T)[None, :]).to(torch.float32)\n",
    "\n",
    "            new_vals_bt = new_vals_bt * (1. - mask_bt) + new_vals_b[:, None] * mask_bt\n",
    "            new_mse_bt  = new_mse_bt  * (1. - mask_bt) + new_mse_b[:, None] * mask_bt\n",
    "\n",
    "        prior_bpd_b = self._prior_bpd(x_0)\n",
    "        total_bpd_b = torch.sum(new_vals_bt, dim=1) + prior_bpd_b\n",
    "\n",
    "        return total_bpd_b, new_vals_bt, prior_bpd_b, new_mse_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ee9f74-6c35-4f19-a47c-08772163f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adopted from https://github.com/rosinality/denoising-diffusion-pytorch with some minor changes.\n",
    "\n",
    "def swish(input):\n",
    "    \"\"\"\n",
    "    activation function. return x*sigmoid(x) \n",
    "    \"\"\"\n",
    "    return input * torch.sigmoid(input)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def variance_scaling_init_(tensor, scale=1, mode=\"fan_avg\", distribution=\"uniform\"):\n",
    "    \"\"\"\n",
    "    mode에 따라 scale이 바뀌고, 그 scale대로 weight tensor를 initialize 해준다.\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n",
    "\n",
    "    if mode == \"fan_in\":\n",
    "        scale /= fan_in\n",
    "\n",
    "    elif mode == \"fan_out\":\n",
    "        scale /= fan_out\n",
    "\n",
    "    else:\n",
    "        scale /= (fan_in + fan_out) / 2\n",
    "\n",
    "    if distribution == \"normal\":\n",
    "        std = math.sqrt(scale)\n",
    "\n",
    "        return tensor.normal_(0, std)\n",
    "\n",
    "    else:\n",
    "        bound = math.sqrt(3 * scale)\n",
    "\n",
    "        return tensor.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    in_channel,\n",
    "    out_channel,\n",
    "    kernel_size,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    bias=True,\n",
    "    scale=1,\n",
    "    mode=\"fan_avg\",\n",
    "):\n",
    "    \"\"\"\n",
    "    convolution layer return하는 함수. \n",
    "    입력한 in_channel, out_channel, kernel_size, stride, padding 에 따라 convolution layer 만든다.\n",
    "    scale, mode에 따라 weight initialize 해주고 bias가 True면 0 bias를 가지도록 return.\n",
    "    \"\"\"\n",
    "    conv = nn.Conv2d(\n",
    "        in_channel, out_channel, kernel_size, stride=stride, padding=padding, bias=bias\n",
    "    )\n",
    "\n",
    "    variance_scaling_init_(conv.weight, scale, mode=mode)\n",
    "\n",
    "    if bias:\n",
    "        nn.init.zeros_(conv.bias)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def linear(in_channel, out_channel, scale=1, mode=\"fan_avg\"):\n",
    "    \"\"\"\n",
    "    linear layer return 하는 함수.\n",
    "    in_channel, out_channel 에 따라 linear layer 만든다\n",
    "    scale, mode에 따라 weight initialize 해주고, 0 bias를 가지도록 해준다.\n",
    "    \"\"\"\n",
    "    lin = nn.Linear(in_channel, out_channel)\n",
    "\n",
    "    variance_scaling_init_(lin.weight, scale, mode=mode)\n",
    "    nn.init.zeros_(lin.bias)\n",
    "\n",
    "    return lin\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    위에서 정의한 swish 함수를 layer 형태로 만든 것\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return swish(input)\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Upsample과 conv2d를 layer로 가지는 nn.Sequential layer\n",
    "    channel을 scale factor만큼 키워준다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        layers = [\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            conv2d(channel, channel, 3, padding=1),\n",
    "        ]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Downsample(nn.Sequential):\n",
    "    \"\"\"\n",
    "    conv2d를 layer로 가지는 nn.Sequential layer. stride가 2라 output image의 크기가 줄어든다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        layers = [conv2d(channel, channel, 3, stride=2, padding=1)]\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip connection을 사용한 network. time embedding 과 dropout을 hyperparameter로 가진다.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, time_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(32, in_channel)\n",
    "        self.activation1 = Swish()\n",
    "        self.conv1 = conv2d(in_channel, out_channel, 3, padding=1)\n",
    "\n",
    "        self.time = nn.Sequential(Swish(), linear(time_dim, out_channel))\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(32, out_channel)\n",
    "        self.activation2 = Swish()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = conv2d(out_channel, out_channel, 3, padding=1, scale=1e-10)\n",
    "\n",
    "        if in_channel != out_channel:\n",
    "            self.skip = conv2d(in_channel, out_channel, 1)\n",
    "\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def forward(self, input, time):\n",
    "        batch = input.shape[0]\n",
    "\n",
    "        out = self.conv1(self.activation1(self.norm1(input)))\n",
    "\n",
    "        out = out + self.time(time).view(batch, -1, 1, 1)\n",
    "\n",
    "        out = self.conv2(self.dropout(self.activation2(self.norm2(out))))\n",
    "\n",
    "        if self.skip is not None:\n",
    "            input = self.skip(input)\n",
    "\n",
    "        return out + input\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention 적용하는 layer\n",
    "    input의 channel 수 4배로 늘린 뒤 torch.chunk 로 q, k, v 나눔. \n",
    "    이때 channel의 수가 3의 배수 아닌데, query와 key의 channel수가 같고, value의 channel 수가 작음\n",
    "    나머지 계산의 dimension 계산은 아래 torch.einsum에 설명\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.GroupNorm(32, in_channel)\n",
    "        self.qkv = conv2d(in_channel, in_channel * 4, 1)\n",
    "        self.out = conv2d(in_channel, in_channel, 1, scale=1e-10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch, channel, height, width = input.shape\n",
    "\n",
    "        norm = self.norm(input)\n",
    "        qkv = self.qkv(norm)\n",
    "        query, key, value = qkv.chunk(3, dim=1)\n",
    "\n",
    "        attn = torch.einsum(\"nchw, ncyx -> nhwyx\", query, key).contiguous() / math.sqrt(\n",
    "            channel\n",
    "        )\n",
    "        attn = attn.view(batch, height, width, -1)\n",
    "        attn = torch.softmax(attn, -1)\n",
    "        attn = attn.view(batch, height, width, height, width)\n",
    "\n",
    "        out = torch.einsum(\"nhwyx, ncyx -> nchw\", attn, input).contiguous()\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out + input\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    transformer의 sinusoidal positional embedding 적용\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim      = dim\n",
    "        half_dim      = self.dim // 2\n",
    "        self.inv_freq = torch.exp(torch.arange(half_dim, dtype=torch.float32) * (-math.log(10000) / (half_dim - 1)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        shape       = input.shape\n",
    "        input       = input.view(-1).to(torch.float32)\n",
    "        sinusoid_in = torch.ger(input, self.inv_freq.to(input.device))\n",
    "        pos_emb     = torch.cat([sinusoid_in.sin(), sinusoid_in.cos()], dim=-1)\n",
    "        pos_emb     = pos_emb.view(*shape, self.dim)\n",
    "        \n",
    "        return pos_emb\n",
    "\n",
    "\n",
    "class ResBlockWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet에서 기본단위로 사용되는 block\n",
    "    Resblock - SelfAttention으로 이루어짐\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, time_dim, dropout, use_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resblocks = ResBlock(in_channel, out_channel, time_dim, dropout)\n",
    "\n",
    "        if use_attention:\n",
    "            self.attention = SelfAttention(out_channel)\n",
    "\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "    def forward(self, input, time):\n",
    "        out = self.resblocks(input, time)\n",
    "\n",
    "        if self.attention is not None:\n",
    "            out = self.attention(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def spatial_fold(input, fold):\n",
    "    \"\"\"\n",
    "    h, w 를 fold만큼 나눈 dim으로 바꿔주고, channel의 dimension을 그만큼 곱함\n",
    "    \"\"\"\n",
    "    if fold == 1:\n",
    "        return input\n",
    "\n",
    "    batch, channel, height, width = input.shape\n",
    "    h_fold = height // fold\n",
    "    w_fold = width // fold\n",
    "\n",
    "    return (\n",
    "        input.view(batch, channel, h_fold, fold, w_fold, fold)\n",
    "        .permute(0, 1, 3, 5, 2, 4)\n",
    "        .reshape(batch, -1, h_fold, w_fold)\n",
    "    )\n",
    "\n",
    "\n",
    "def spatial_unfold(input, unfold):\n",
    "    \"\"\"\n",
    "    spatial_fold와 반대.\n",
    "    h, w를 unfold만큼 곱한 dim으로 바꿔주고, channel의 dimension을 그만큼 나눔\n",
    "    \"\"\"\n",
    "    if unfold == 1:\n",
    "        return input\n",
    "\n",
    "    batch, channel, height, width = input.shape\n",
    "    h_unfold = height * unfold\n",
    "    w_unfold = width * unfold\n",
    "\n",
    "    return (\n",
    "        input.view(batch, -1, unfold, unfold, height, width)\n",
    "        .permute(0, 1, 4, 2, 5, 3)\n",
    "        .reshape(batch, -1, h_unfold, w_unfold)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    학습에 사용할 전체 모델.\n",
    "    dimension과 구조는 위의 markdown 그림 참조\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        channel,\n",
    "        channel_multiplier,\n",
    "        n_res_blocks,\n",
    "        attn_strides,\n",
    "        dropout=0,\n",
    "        fold=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fold = fold\n",
    "\n",
    "        time_dim = channel * 4\n",
    "        \n",
    "        self.enc_data = nn.Sequential(linear(numofcluster*100+18,3072))\n",
    "        \n",
    "        self.dec_data = nn.Sequential(linear(3072,numofcluster*100))\n",
    "\n",
    "        n_block = len(channel_multiplier)\n",
    "        \n",
    "\n",
    "        self.time = nn.Sequential(\n",
    "            TimeEmbedding(channel),\n",
    "            linear(channel, time_dim),\n",
    "            Swish(),\n",
    "            linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        down_layers   = [conv2d(in_channel * (fold ** 2), channel, 3, padding=1)]\n",
    "        feat_channels = [channel]\n",
    "        in_channel    = channel\n",
    "        for i in range(n_block):\n",
    "            for _ in range(n_res_blocks):\n",
    "                channel_mult = channel * channel_multiplier[i]\n",
    "\n",
    "                down_layers.append(\n",
    "                    ResBlockWithAttention(\n",
    "                        in_channel,\n",
    "                        channel_mult,\n",
    "                        time_dim,\n",
    "                        dropout,\n",
    "                        use_attention=2 ** i in attn_strides,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                feat_channels.append(channel_mult)\n",
    "                in_channel = channel_mult\n",
    "\n",
    "            if i != n_block - 1:\n",
    "                down_layers.append(Downsample(in_channel))\n",
    "                feat_channels.append(in_channel)\n",
    "\n",
    "        self.down = nn.ModuleList(down_layers)\n",
    "\n",
    "        self.mid = nn.ModuleList(\n",
    "            [\n",
    "                ResBlockWithAttention(\n",
    "                    in_channel,\n",
    "                    in_channel,\n",
    "                    time_dim,\n",
    "                    dropout=dropout,\n",
    "                    use_attention=True,\n",
    "                ),\n",
    "                ResBlockWithAttention(\n",
    "                    in_channel, in_channel, time_dim, dropout=dropout\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        up_layers = []\n",
    "        for i in reversed(range(n_block)):\n",
    "            for _ in range(n_res_blocks + 1):\n",
    "                channel_mult = channel * channel_multiplier[i]\n",
    "\n",
    "                up_layers.append(\n",
    "                    ResBlockWithAttention(\n",
    "                        in_channel + feat_channels.pop(),\n",
    "                        channel_mult,\n",
    "                        time_dim,\n",
    "                        dropout=dropout,\n",
    "                        use_attention=2 ** i in attn_strides,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                in_channel = channel_mult\n",
    "\n",
    "            if i != 0:\n",
    "                up_layers.append(Upsample(in_channel))\n",
    "\n",
    "        self.up = nn.ModuleList(up_layers)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_channel),\n",
    "            Swish(),\n",
    "            conv2d(in_channel, 3 * (fold ** 2), 3, padding=1, scale=1e-10),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, condition,time):\n",
    "        \n",
    "        batch = input.shape[0]\n",
    "        x = torch.cat((input, condition),dim=1)\n",
    "        \n",
    "        x=self.enc_data(x)\n",
    "        time_embed = self.time(time)\n",
    "        feats = []\n",
    "        out = x.view(batch, 3,32,32)\n",
    "        \n",
    "        out = spatial_fold(out, self.fold)\n",
    "        \n",
    "        for layer in self.down:\n",
    "            if isinstance(layer, ResBlockWithAttention):\n",
    "                out = layer(out, time_embed)\n",
    "\n",
    "            else:\n",
    "                out = layer(out)\n",
    "\n",
    "            feats.append(out)\n",
    "\n",
    "        for layer in self.mid:\n",
    "            out = layer(out, time_embed)\n",
    "\n",
    "        for layer in self.up:\n",
    "            if isinstance(layer, ResBlockWithAttention):\n",
    "                out = layer(torch.cat((out, feats.pop()), 1), time_embed)\n",
    "\n",
    "            else:\n",
    "                out = layer(out)\n",
    "\n",
    "        out = self.out(out)\n",
    "        out = spatial_unfold(out, self.fold)\n",
    "        \n",
    "        out = out.view(batch, 3072)\n",
    "        kk = self.dec_data(out)\n",
    "        \n",
    "        return kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30eb0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[[5.  2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 2141)\n",
      "Epoch [100/100], Loss: 0.0011282850755378604\n",
      "4.0\n",
      "[[2.5 4.  4.  ... 4.  2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 1457)\n",
      "Epoch [100/100], Loss: 0.003153582103550434\n",
      "1.0\n",
      "[[2.5 5.  2.5 ... 5.  4.  3. ]\n",
      " [2.5 5.  2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 3.  1.  2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 916)\n",
      "Epoch [100/100], Loss: 0.006165783386677504\n",
      "2.0\n",
      "[[2.5 2.5 2.5 ... 3.  2.5 2.5]\n",
      " [2.5 3.  2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 988)\n",
      "Epoch [100/100], Loss: 0.004485299810767174\n",
      "3.0\n",
      "[[3.  4.  2.5 ... 3.  4.  2.5]\n",
      " [2.5 3.  5.  ... 3.  2.5 2.5]\n",
      " [2.  2.5 2.5 ... 3.  2.  2.5]\n",
      " ...\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]\n",
      " [2.5 2.5 2.5 ... 2.5 2.5 2.5]] (3706, 538)\n",
      "Epoch [100/100], Loss: 0.012117566540837288\n",
      "0.0\n",
      "tensor([[2.5272, 2.5201, 2.5126,  ..., 2.5117, 2.5166, 2.5046],\n",
      "        [2.5242, 2.5216, 2.5004,  ..., 2.5205, 2.5066, 2.5054],\n",
      "        [2.5238, 2.5218, 2.5002,  ..., 2.5154, 2.5056, 2.5073],\n",
      "        ...,\n",
      "        [2.5259, 2.5282, 2.5024,  ..., 2.5187, 2.5118, 2.5018],\n",
      "        [2.5165, 2.5228, 2.5156,  ..., 2.5073, 2.5027, 2.5142],\n",
      "        [2.5215, 2.5164, 2.5085,  ..., 2.5227, 2.4977, 2.5010]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.4714, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.5626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "4.0\n",
      "tensor([[2.5614, 2.5428, 2.5769,  ..., 2.5488, 2.5472, 2.5701],\n",
      "        [2.5493, 2.5297, 2.5602,  ..., 2.5487, 2.5222, 2.5494],\n",
      "        [2.5379, 2.5267, 2.5530,  ..., 2.5446, 2.5168, 2.5428],\n",
      "        ...,\n",
      "        [2.5464, 2.5354, 2.5716,  ..., 2.5356, 2.5300, 2.5665],\n",
      "        [2.5421, 2.5205, 2.5448,  ..., 2.5393, 2.5031, 2.5400],\n",
      "        [2.5497, 2.5277, 2.5504,  ..., 2.5441, 2.5179, 2.5455]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.4775, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.6831, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "1.0\n",
      "tensor([[2.7272, 3.1832, 2.7815,  ..., 2.8275, 2.7929, 3.0365],\n",
      "        [2.5318, 2.6356, 2.5659,  ..., 2.5673, 2.5073, 2.6084],\n",
      "        [2.5186, 2.6502, 2.5691,  ..., 2.5641, 2.5013, 2.5910],\n",
      "        ...,\n",
      "        [2.5477, 2.6356, 2.5020,  ..., 2.5254, 2.4856, 2.5913],\n",
      "        [2.5277, 2.7032, 2.5227,  ..., 2.6151, 2.5038, 2.6313],\n",
      "        [2.5834, 2.7144, 2.5835,  ..., 2.5734, 2.5486, 2.5867]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.1797, device='cuda:0', grad_fn=<MinBackward1>) tensor(3.8981, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "2.0\n",
      "tensor([[2.5470, 2.5567, 2.6311,  ..., 2.5690, 2.5568, 2.5948],\n",
      "        [2.5410, 2.5504, 2.6037,  ..., 2.5677, 2.5470, 2.5752],\n",
      "        [2.5365, 2.5187, 2.5934,  ..., 2.5411, 2.5599, 2.6004],\n",
      "        ...,\n",
      "        [2.5188, 2.5342, 2.5976,  ..., 2.5289, 2.5516, 2.5614],\n",
      "        [2.5156, 2.5486, 2.5872,  ..., 2.5612, 2.5552, 2.5601],\n",
      "        [2.5237, 2.5641, 2.5971,  ..., 2.5471, 2.5612, 2.5685]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.4586, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.8023, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "3.0\n",
      "tensor([[2.6177, 2.7161, 3.2234,  ..., 2.6811, 2.9372, 2.9795],\n",
      "        [2.5781, 2.6398, 2.8827,  ..., 2.6773, 2.7408, 2.7705],\n",
      "        [2.5860, 2.6457, 2.8822,  ..., 2.6670, 2.6934, 2.7236],\n",
      "        ...,\n",
      "        [2.5344, 2.6045, 2.8500,  ..., 2.6390, 2.6687, 2.6676],\n",
      "        [2.5322, 2.6144, 2.7560,  ..., 2.5581, 2.6851, 2.6533],\n",
      "        [2.5644, 2.6010, 2.8649,  ..., 2.6245, 2.7008, 2.7480]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(2.2831, device='cuda:0', grad_fn=<MinBackward1>) tensor(4.1831, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = encoding_dim\n",
    "        in_channels = input_size\n",
    "        hidden_dims = [512, 256, 128]\n",
    "        # Encoder\n",
    "        modules = []\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_channels, h_dim),\n",
    "                    nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], self.latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], self.latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.latent_dim, hidden_dims[-1]),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        )\n",
    "        hidden_dims.reverse()\n",
    "        \n",
    "        for i in range(len(hidden_dims)-1):            \n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
    "                    nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dims[-1], input_size),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mu = self.fc_mu(encoded)\n",
    "        log_var = self.fc_var(encoded)\n",
    "        encoded = self.reparameterize(mu, log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps*std + mu\n",
    "\n",
    "\n",
    "df = new_data\n",
    "df_dict = {}\n",
    "df_encoded_dict = {}\n",
    "\n",
    "movieid = df.iloc[:-1,0]\n",
    "group_gmm_values = df.loc[df.index[-1], :]\n",
    "\n",
    "for group_value in group_gmm_values.unique():\n",
    "    group_df = df.loc[:, df.loc[df.index[-1], :] == group_value]\n",
    "    \n",
    "    df_dict[group_value] = group_df.iloc[:-1,:] # original data save\n",
    "    \n",
    "VAEs = {}\n",
    "clusternum = {}\n",
    "for group_num in df_dict.keys():\n",
    "    if type(group_num)!=str:\n",
    "        print(group_num)\n",
    "        data = df_dict[group_num]\n",
    "        data = data.replace(np.nan, 2.5)\n",
    "        input_data = data.values\n",
    "        df_dict[group_num] = input_data\n",
    "        del(data)\n",
    "        gc.collect()\n",
    "        \n",
    "        # Convert the input data to a PyTorch tensor\n",
    "        print(input_data, input_data.shape)\n",
    "        clusternum[group_num] = input_data.shape[1]\n",
    "        input_tensor = torch.from_numpy(input_data/5).float().to(\"cuda\")\n",
    "\n",
    "        # Initialize the autoencoder model\n",
    "        input_size = input_data.shape[1] \n",
    "        encoding_dim = input_size  \n",
    "        VAE_new = VAE(input_size, 100).to(\"cuda\")\n",
    "        if torch.cuda.device_count()>1:\n",
    "            VAE_new = nn.DataParallel(VAE_new)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(VAE_new.parameters(), lr=0.0001)\n",
    "        \n",
    "        # Training the autoencoder\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            _, outputs = VAE_new(input_tensor)\n",
    "            loss = criterion(outputs, input_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch%100 == 99:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        VAEs[group_num]=VAE_new\n",
    "\n",
    "        \n",
    "for group_num in df_dict.keys():\n",
    "    if type(group_num)!=str:\n",
    "        print(group_num)\n",
    "        encoded, decoded = VAEs[group_num](torch.from_numpy(df_dict[group_num]/5).float().to(\"cuda\"))\n",
    "        df_encoded_dict[group_num] = encoded\n",
    "        decoded = decoded*5\n",
    "        print(decoded)\n",
    "        print(decoded.min(), decoded.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f3e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3706, 500])\n",
      "torch.Size([3706, 18])\n",
      "middle =  tensor(-0.6660, device='cuda:0', grad_fn=<DivBackward0>) halfrange = tensor(7.9600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "data_condition = \"./data/data_delete_nogenres_1m.csv\"\n",
    "cond_data = pd.read_csv(data_condition)\n",
    "\n",
    "for num, j in enumerate(movieid):\n",
    "    temp_cond = torch.tensor(cond_data.loc[cond_data[\"movieId\"]==int(j)].iloc[:,2:].values,dtype = torch.float32)\n",
    "    if temp_cond.shape[0]==0:\n",
    "        print(j)\n",
    "    if num==0:\n",
    "        condition = temp_cond\n",
    "        \n",
    "    else:\n",
    "        condition = torch.cat((condition, temp_cond), dim=0)\n",
    "        \n",
    "for i in df_encoded_dict.keys():\n",
    "    try:\n",
    "        input_data = torch.cat((input_data, df_encoded_dict[i]),dim=1)\n",
    "\n",
    "\n",
    "    except:\n",
    "        input_data = df_encoded_dict[i]\n",
    "\n",
    "print(input_data.shape)\n",
    "print(condition.shape)\n",
    "\n",
    "middle_input = (input_data.max()+input_data.min())/2\n",
    "halfrange = (input_data.max()-input_data.min()) / 2\n",
    "\n",
    "input_data = (input_data - middle_input) / halfrange\n",
    "\n",
    "print(\"middle = \", middle_input, \"halfrange =\", halfrange)\n",
    "\n",
    "print(input_data.max(), input_data.min())\n",
    "Totaldata = (input_data, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0293b81-9319-40f5-969e-b90f47529e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(conf, inputdata):\n",
    "    if conf.dataset.name == 'movielens':\n",
    "        data, condition = inputdata\n",
    "        data = data.to(\"cpu\").detach()\n",
    "        condition = condition.detach()\n",
    "        datatuple = (data, condition)\n",
    "        dataset = TensorDataset(*datatuple)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_size = 3000\n",
    "        train_set = Subset(dataset, range(train_size))\n",
    "        valid_set = Subset(dataset, range(train_size, data.shape[0]))\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6a0fc4-d8d5-47f2-92eb-248cd8a82b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## util 복붙\n",
    "\n",
    "class obj(object):\n",
    "    \"\"\"\n",
    "    config가 json 형식으로 되어있습니다.\n",
    "    json 파일의 데이터를 받아오는 형식을 바꿔주는 함수입니다. \n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        for a, b in d.items():\n",
    "            if isinstance(b, (list, tuple)):\n",
    "               setattr(self, a, [obj(x) if isinstance(x, dict) else x for x in b])\n",
    "            else:\n",
    "               setattr(self, a, obj(b) if isinstance(b, dict) else b)\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.9999):\n",
    "    \"\"\"\n",
    "    아래 DDP class에서 EMA를 진행하기 위한 함수입니다.\n",
    "    model1의 parameter을 기존 param*decay + model2의 param*(1-decay)로 update 합니다.\n",
    "    \"\"\"\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def samples_fn(model, diffusion, shape, condition):\n",
    "    \"\"\"\n",
    "    sample image의 현재 범위는 -1~1입니다.\n",
    "    이를 image화 시키려먼 0~1 사이의 범위로 만들어주어야 하고, 이를 수행하는 함수입니다.\n",
    "    \"\"\"\n",
    "    samples = diffusion.p_sample_loop(model=model,\n",
    "                                      shape=shape,\n",
    "                                      noise_fn=torch.randn,\n",
    "                                     condition = condition)\n",
    "    return {\n",
    "      'samples': (samples + 1)/2\n",
    "    }\n",
    "\n",
    "\n",
    "def progressive_samples_fn(model, diffusion, shape, device, include_x0_pred_freq=50, ret_type = \"img\", condition= torch.zeros(19)):\n",
    "    \"\"\"\n",
    "    위 samples_fn과 역할이 동일합니다.\n",
    "    \"\"\"\n",
    "    samples, progressive_samples, pro_list = diffusion.p_sample_loop_progressive(\n",
    "        model=model,\n",
    "        shape=shape,\n",
    "        noise_fn=torch.randn,\n",
    "        device=device,\n",
    "        include_x0_pred_freq=include_x0_pred_freq,\n",
    "        ret_type = ret_type,\n",
    "        condition = condition\n",
    "    )\n",
    "    if ret_type == \"list\":\n",
    "        for i in range(len(samples)):\n",
    "            samples[i] = (samples[i] + 1)/2\n",
    "        for j in range(len(pro_list)):\n",
    "            pro_list[j] = (pro_list[j]+1)/2\n",
    "        return {'samples': samples, 'progressive_samples': (progressive_samples + 1)/2, \"pro_list\" : pro_list}\n",
    "    elif ret_type == \"img\":\n",
    "        samples = (samples +1)/2\n",
    "        return {'samples': samples, 'progressive_samples': (progressive_samples + 1)/2}\n",
    "\n",
    "\n",
    "def bpd_fn(model, diffusion, x, condition):\n",
    "    \"\"\"\n",
    "    복원 오차 계산하는 함수. diffusion.calc_bpd_loop 참고\n",
    "    \"\"\"\n",
    "    total_bpd_b, terms_bpd_bt, prior_bpd_b, mse_bt = diffusion.calc_bpd_loop(model=model, x_0=x, clip_denoised=True, condition = condition)\n",
    "\n",
    "    return {\n",
    "      'total_bpd': total_bpd_b,\n",
    "      'terms_bpd': terms_bpd_bt,\n",
    "      'prior_bpd': prior_bpd_b,\n",
    "      'mse': mse_bt\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(val_loader, model, diffusion, condition):\n",
    "    \"\"\"\n",
    "    validation dataset에 대해 bpd, mse 계산하는 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    bpd = []\n",
    "    mse = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(iter(val_loader)):\n",
    "            x       = x\n",
    "            metrics = bpd_fn(model, diffusion, x, condition)\n",
    "\n",
    "            bpd.append(metrics['total_bpd'].view(-1, 1))\n",
    "            mse.append(metrics['mse'].view(-1, 1))\n",
    "\n",
    "        bpd = torch.cat(bpd, dim=0).mean()\n",
    "        mse = torch.cat(mse, dim=0).mean()\n",
    "\n",
    "    return bpd, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1556529d-326b-469a-b7d2-3515bd844058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDP(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, conf, inputdata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conf  = conf\n",
    "        self.save_hyperparameters()\n",
    "        self.input = inputdata\n",
    "\n",
    "        self.model = UNet(self.conf.model.in_channel,\n",
    "                          self.conf.model.channel,\n",
    "                          channel_multiplier=self.conf.model.channel_multiplier,\n",
    "                          n_res_blocks=self.conf.model.n_res_blocks,\n",
    "                          attn_strides=self.conf.model.attn_strides,\n",
    "                          dropout=self.conf.model.dropout,\n",
    "                          fold=self.conf.model.fold\n",
    "                          )\n",
    "        self.ema   = UNet(self.conf.model.in_channel,\n",
    "                          self.conf.model.channel,\n",
    "                          channel_multiplier=self.conf.model.channel_multiplier,\n",
    "                          n_res_blocks=self.conf.model.n_res_blocks,\n",
    "                          attn_strides=self.conf.model.attn_strides,\n",
    "                          dropout=self.conf.model.dropout,\n",
    "                          fold=self.conf.model.fold\n",
    "                          )\n",
    "\n",
    "        self.betas = make_beta_schedule(schedule=self.conf.model.schedule.type,\n",
    "                                        start=self.conf.model.schedule.beta_start,\n",
    "                                        end=self.conf.model.schedule.beta_end,\n",
    "                                        n_timestep=self.conf.model.schedule.n_timestep)\n",
    "\n",
    "        self.diffusion = GaussianDiffusion(betas=self.betas,\n",
    "                                           model_mean_type=self.conf.model.mean_type,\n",
    "                                           model_var_type=self.conf.model.var_type,\n",
    "                                           loss_type=self.conf.model.loss_type)\n",
    "        \n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_set, self.valid_set = get_train_data(self.conf, self.input)\n",
    "        \n",
    "\n",
    "    def forward(self, x, condition):\n",
    "\n",
    "        return self.diffusion.p_sample_loop(self.model, x.shape, condition = condition)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        if self.conf.training.optimizer.type == 'adam':\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.conf.training.optimizer.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "\n",
    "        img, cond = batch[0], batch[1]\n",
    "        time   = (torch.rand(img.shape[0]) * 1000).type(torch.int64).to(img.device)\n",
    "        loss   = self.diffusion.training_losses(self.model, img, cond, time).mean()\n",
    "\n",
    "        accumulate(self.ema, self.model.module if isinstance(self.model, nn.DataParallel) else self.model, 0.9999)\n",
    "\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_loader = DataLoader(self.train_set,\n",
    "                                  batch_size=self.conf.training.dataloader.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=self.conf.training.dataloader.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=self.conf.training.dataloader.drop_last)\n",
    "        return train_loader\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        img, cond = batch[0], batch[1]\n",
    "        time   = (torch.rand(img.shape[0]) * 1000).type(torch.int64).to(img.device)\n",
    "        loss   = self.diffusion.training_losses(self.ema, img, cond, time).mean()\n",
    "\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss         = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "\n",
    "        \n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_loader = DataLoader(self.valid_set,\n",
    "                                  batch_size=self.conf.validation.dataloader.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=self.conf.validation.dataloader.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  drop_last=self.conf.validation.dataloader.drop_last)\n",
    "\n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eebb304-481e-4825-aa23-84243cc73d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"train\": False,\n",
    "        \"config\": 'config/diffusion_movielens.json',\n",
    "        \"ckpt_dir\": './ckpts',\n",
    "        \"ckpt_freq\": 5,\n",
    "        \"n_gpu\": 1,\n",
    "        \"model_dir\": './ckpts/last.ckpt',\n",
    "        \"sample_dir\": 'samples',\n",
    "        \"prog_sample_freq\": 100,\n",
    "        \"n_samples\": 100\n",
    "    })\n",
    "\n",
    "    \n",
    "path_to_config = args.config\n",
    "with open(path_to_config, 'r') as f:\n",
    "    conf = json.load(f)\n",
    "\n",
    "conf = obj(conf)\n",
    "denoising_diffusion_model = DDP(conf, Totaldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300c476e-b937-4388-a662-939a85dc7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pl.__version__ == '0.8.5':\n",
    "    checkpoint_callback = ModelCheckpoint(filepath=os.path.join(args.ckpt_dir, 'ddp_{epoch:02d}-{val_loss:.2f}'),\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=True,\n",
    "                                      save_last=True,\n",
    "                                      save_top_k=-1,\n",
    "                                      save_weights_only=True,\n",
    "                                      mode='auto',\n",
    "                                      period=args.ckpt_freq,\n",
    "                                      prefix='')\n",
    "    \n",
    "\n",
    "#     except:\n",
    "#         print('The present library version of pytorch lightning is ',pl.__version__)\n",
    "#         print('Please check if library requirements are satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06693053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | UNet              | 37 M  \n",
      "1 | ema       | UNet              | 37 M  \n",
      "2 | diffusion | GaussianDiffusion | 0     \n",
      "C:\\Users\\User\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:478: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ccbcf8a4ca48c099cd0115ff865077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py:370\u001b[0m, in \u001b[0;36mTrainerTrainLoopMixin.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# -----------------\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# RUN TNG EPOCH\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# -----------------\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py:439\u001b[0m, in \u001b[0;36mTrainerTrainLoopMixin.run_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# run epoch\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (batch, is_last_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile_iterable(\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(_with_is_last(train_dataloader)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_train_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m ):\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# stop epoch if we limited the number of training batches\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_training_batches:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\profiler\\profilers.py:64\u001b[0m, in \u001b[0;36mBaseProfiler.profile_iterable\u001b[1;34m(self, iterable, action_name)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart(action_name)\n\u001b[1;32m---> 64\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop(action_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py:1012\u001b[0m, in \u001b[0;36m_with_is_last\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m   1011\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterable)\n\u001b[1;32m-> 1012\u001b[0m last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;66;03m# yield last and has next\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1186\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1142\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1142\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 990\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(fast_dev_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      2\u001b[0m                      gpus\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu,\n\u001b[0;32m      3\u001b[0m                      max_steps\u001b[38;5;241m=\u001b[39mconf\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mn_iter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m                      distributed_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m                     )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoising_diffusion_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:997\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# 1 gpu or dp option triggers training using DP module\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;66;03m# easier to avoid NCCL issues\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dp:\n\u001b[1;32m--> 997\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdp_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_horovod:\n\u001b[0;32m   1000\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhorovod_train(model)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\distrib_parts.py:270\u001b[0m, in \u001b[0;36mTrainerDPMixin.dp_train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    266\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_gpu)\n\u001b[0;32m    268\u001b[0m model \u001b[38;5;241m=\u001b[39m LightningDataParallel(model, device_ids\u001b[38;5;241m=\u001b[39mdevice_ids)\n\u001b[1;32m--> 270\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pretrain_routine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m model\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m model_autocast_original_forward\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1213\u001b[0m, in \u001b[0;36mTrainer.run_pretrain_routine\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m   1210\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# CORE TRAINING LOOP\u001b[39;00m\n\u001b[1;32m-> 1213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py:402\u001b[0m, in \u001b[0;36mTrainerTrainLoopMixin.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_keyboard_interrupt()\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py:878\u001b[0m, in \u001b[0;36mTrainerTrainLoopMixin.run_training_teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mon_train_end()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuccess\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;66;03m# summarize profile results\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:12\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rank_zero_only\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:166\u001b[0m, in \u001b[0;36mTensorBoardLogger.finalize\u001b[1;34m(self, status)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinalize\u001b[39m(\u001b[38;5;28mself\u001b[39m, status: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:12\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rank_zero_only\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:162\u001b[0m, in \u001b[0;36mTensorBoardLogger.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# save the metatags file\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:377\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[1;34m(config_yaml, hparams)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hparams, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_yaml, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m--> 377\u001b[0m     \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\__init__.py:253\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(data, stream, Dumper, **kwds)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(data, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, Dumper\u001b[38;5;241m=\u001b[39mDumper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    Serialize a Python object into a YAML stream.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    If stream is None, return the produced string instead.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdump_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDumper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDumper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\__init__.py:241\u001b[0m, in \u001b[0;36mdump_all\u001b[1;34m(documents, stream, Dumper, default_style, default_flow_style, canonical, indent, width, allow_unicode, line_break, encoding, explicit_start, explicit_end, version, tags, sort_keys)\u001b[0m\n\u001b[0;32m    239\u001b[0m     dumper\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m--> 241\u001b[0m         \u001b[43mdumper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     dumper\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\representer.py:28\u001b[0m, in \u001b[0;36mBaseRepresenter.represent\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresent\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     27\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresent_data(data)\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresented_objects \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_keeper \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\serializer.py:54\u001b[0m, in \u001b[0;36mSerializer.serialize\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(DocumentStartEvent(explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_explicit_start,\n\u001b[0;32m     52\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_version, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_tags))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_node(node)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(DocumentEndEvent(explicit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_explicit_end))\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialized_nodes \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\serializer.py:108\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[1;34m(self, node, parent, index)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialize_node(key, node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 108\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(MappingEndEvent())\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascend_resolver()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\serializer.py:98\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[1;34m(self, node, parent, index)\u001b[0m\n\u001b[0;32m     96\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(SequenceEndEvent())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\serializer.py:98\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[1;34m(self, node, parent, index)\u001b[0m\n\u001b[0;32m     96\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit(SequenceEndEvent())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\serializer.py:100\u001b[0m, in \u001b[0;36mSerializer.serialize_node\u001b[1;34m(self, node, parent, index)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserialize_node(item, node, index)\n\u001b[0;32m     99\u001b[0m         index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSequenceEndEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MappingNode):\n\u001b[0;32m    102\u001b[0m     implicit \u001b[38;5;241m=\u001b[39m (node\u001b[38;5;241m.\u001b[39mtag\n\u001b[0;32m    103\u001b[0m                 \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve(MappingNode, node\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:115\u001b[0m, in \u001b[0;36mEmitter.emit\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_more_events():\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:374\u001b[0m, in \u001b[0;36mEmitter.expect_first_block_sequence_item\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpect_first_block_sequence_item\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_block_sequence_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:384\u001b[0m, in \u001b[0;36mEmitter.expect_block_sequence_item\u001b[1;34m(self, first)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_indicator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, indention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpect_block_sequence_item)\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:242\u001b[0m, in \u001b[0;36mEmitter.expect_node\u001b[1;34m(self, root, sequence, mapping, simple_key)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, (ScalarEvent, CollectionStartEvent)):\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_anchor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, ScalarEvent):\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpect_scalar()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:473\u001b[0m, in \u001b[0;36mEmitter.process_tag\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent, ScalarEvent):\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_scalar_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical \u001b[38;5;129;01mor\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    475\u001b[0m         ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mimplicit[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    476\u001b[0m                 \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mimplicit[\u001b[38;5;241m1\u001b[39m]))):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepared_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:496\u001b[0m, in \u001b[0;36mEmitter.choose_scalar_style\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_scalar_style\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\diffusion\\lib\\site-packages\\yaml\\emitter.py:668\u001b[0m, in \u001b[0;36mEmitter.analyze_scalar\u001b[1;34m(self, scalar)\u001b[0m\n\u001b[0;32m    665\u001b[0m previous_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    667\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 668\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscalar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    669\u001b[0m     ch \u001b[38;5;241m=\u001b[39m scalar[index]\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;66;03m# Check for indicators.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(fast_dev_run=False,\n",
    "                     gpus=args.n_gpu,\n",
    "                     max_steps=conf.training.n_iter,\n",
    "                     precision=conf.model.precision,\n",
    "                     gradient_clip_val=1.,\n",
    "                     progress_bar_refresh_rate=1,\n",
    "                     checkpoint_callback=checkpoint_callback,\n",
    "                     check_val_every_n_epoch=10,\n",
    "                     distributed_backend='dp'\n",
    "                    )\n",
    "\n",
    "\n",
    "trainer.fit(denoising_diffusion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5047af0-98e1-4d6b-b6a5-91f972e52f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(denoising_diffusion_model.state_dict(), f\"./trained_ckpt/model_1m_{clustertype}={numofcluster}_VAE_sig.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc9ad83-d955-4971-8a54-138e2d99d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, key in enumerate(VAEs.keys()):\n",
    "    torch.save(VAEs[key].state_dict(),f\"./trained_ckpt/model_1m_{clustertype}={numofcluster}_VAE{key}_sig.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df5acc-3b76-4804-8f25-1de3e6f0e0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
